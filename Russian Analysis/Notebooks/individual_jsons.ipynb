{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, glob\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    count=0\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to savedata to\n",
    "        save_file = file[9:]\n",
    "        print(save_file)\n",
    "        save_dir = \"../ftrs/\" + save_file.replace('.json', '.ftr')\n",
    "        print(save_dir)\n",
    "\n",
    "        for line in f:\n",
    "            if count > 10000:\n",
    "                break\n",
    "            count = 0\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if count == 0:\n",
    "                # Retweet?\n",
    "                if data[\"text\"][:2] != \"RT\":\n",
    "                    #location\n",
    "                    user_location = data[\"user\"][\"location\"]\n",
    "                    #Tweet ID\n",
    "                    tweet_id = data[\"id_str\"]\n",
    "                    #tweet text\n",
    "                    if \"extended_tweet\" in data:\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                    else:\n",
    "                        text = data[\"text\"]\n",
    "\n",
    "                    rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "\n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\",\"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from prepro.ipynb\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json, os, glob\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import import_ipynb\n",
    "import prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for pronouns\n",
    "def whole_word_checker(pronouns):\n",
    "    return re.compile(r'\\b({0})\\b'.format(pronouns), flags=re.IGNORECASE).search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun_checker(text):\n",
    "    vocab = [\"я\", \"мы\", \"ты\", \"нас\", \"вы\", \"тебя\", \"меня\",\"мне\", \"вас\", \"их\", \"они\"]\n",
    "    check = []\n",
    "    for i in vocab:\n",
    "        check.append(whole_word_checker(i)(text.lower()))\n",
    "\n",
    "    count = 0\n",
    "    for i in check:\n",
    "        if i == None:\n",
    "            count+=1\n",
    "    \n",
    "    if count < len(vocab):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "if pronouns == True:\n",
    "    additional_dir = '-pre-я-pronoun.ftr'\n",
    "else:\n",
    "    additional_dir = '-without-мне-pronouns.ftr'\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "    rows = []\n",
    "    links = []\n",
    "    count=0\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to save data to\n",
    "        save_dir = file.replace('.json', additional_dir)\n",
    "        print(save_dir)\n",
    "\n",
    "        for line in f:\n",
    "            if count > 10000:\n",
    "                break\n",
    "            count = 0\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if count == 0:\n",
    "                # Retweet?\n",
    "                if data[\"text\"][:2] != \"RT\":\n",
    "                    #username\n",
    "                    user_id = data[\"user\"][\"screen_name\"]\n",
    "                    #location\n",
    "                    user_location = data[\"user\"][\"location\"]\n",
    "                    #Tweet ID\n",
    "                    tweet_id = data[\"id_str\"]\n",
    "                    #Hashtags\n",
    "                    hashtags = data[\"entities\"][\"hashtags\"]\n",
    "                    # Retweet?\n",
    "                    retweet = data[\"retweeted\"]\n",
    "                    #tweet text\n",
    "                    if \"extended_tweet\" in data:\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "                    else:\n",
    "                        text = data[\"text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "                            \n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\", \"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\", save_dir)\n",
    "\n",
    "display(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202757\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116222\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\2020-03-01.json\n",
      "DONE\n",
      "\\2020-03-02.json\n",
      "DONE\n",
      "\\2020-03-03.json\n",
      "DONE\n",
      "\\2020-03-04.json\n",
      "DONE\n",
      "\\2020-03-05.json\n",
      "DONE\n",
      "\\2020-03-06.json\n",
      "DONE\n",
      "\\2020-03-07.json\n",
      "DONE\n",
      "\\2020-03-08.json\n",
      "DONE\n",
      "\\2020-03-09.json\n",
      "DONE\n",
      "\\2020-03-10.json\n",
      "DONE\n",
      "\\2020-03-11.json\n",
      "DONE\n",
      "\\2020-03-12.json\n",
      "DONE\n",
      "\\2020-03-13.json\n",
      "DONE\n",
      "\\2020-03-14.json\n",
      "DONE\n",
      "\\2020-03-15.json\n",
      "DONE\n",
      "\\2020-03-16.json\n",
      "DONE\n",
      "\\2020-03-17.json\n",
      "DONE\n",
      "\\2020-03-18.json\n",
      "DONE\n",
      "\\2020-03-19.json\n",
      "DONE\n",
      "\\2020-03-20.json\n",
      "DONE\n",
      "\\2020-03-21.json\n",
      "DONE\n",
      "\\2020-03-22.json\n",
      "DONE\n",
      "\\2020-03-23.json\n",
      "DONE\n",
      "\\2020-03-24.json\n",
      "DONE\n",
      "\\2020-03-25.json\n",
      "DONE\n",
      "\\2020-03-26.json\n",
      "DONE\n",
      "\\2020-03-27.json\n",
      "DONE\n",
      "\\2020-03-28.json\n",
      "DONE\n",
      "\\2020-03-29.json\n",
      "DONE\n",
      "\\2020-03-30.json\n",
      "DONE\n",
      "\\2020-03-31.json\n",
      "DONE\n",
      "\\2020-04-01.json\n",
      "DONE\n",
      "\\2020-04-02.json\n",
      "DONE\n",
      "\\2020-04-03.json\n",
      "DONE\n",
      "\\2020-04-04.json\n",
      "DONE\n",
      "\\2020-04-05.json\n",
      "DONE\n",
      "\\2020-04-06.json\n",
      "DONE\n",
      "\\2020-04-07.json\n",
      "DONE\n",
      "\\2020-04-08.json\n",
      "DONE\n",
      "\\2020-04-09.json\n",
      "DONE\n",
      "\\2020-04-10.json\n",
      "DONE\n",
      "\\2020-04-11.json\n",
      "DONE\n",
      "\\2020-04-12.json\n",
      "DONE\n",
      "\\2020-04-13.json\n",
      "DONE\n",
      "\\2020-04-14.json\n",
      "DONE\n",
      "\\2020-04-15.json\n",
      "DONE\n",
      "\\2020-04-16.json\n",
      "DONE\n",
      "\\2020-04-17.json\n",
      "DONE\n",
      "\\2020-04-18.json\n",
      "DONE\n",
      "\\2020-04-19.json\n",
      "DONE\n",
      "\\2020-04-20.json\n",
      "DONE\n",
      "\\2020-04-21.json\n",
      "DONE\n",
      "\\2020-04-22.json\n",
      "DONE\n",
      "\\2020-04-23.json\n",
      "DONE\n",
      "\\2020-04-24.json\n",
      "DONE\n",
      "\\2020-04-25.json\n",
      "DONE\n",
      "\\2020-04-26.json\n",
      "DONE\n",
      "\\2020-04-27.json\n",
      "DONE\n",
      "\\2020-04-28.json\n",
      "DONE\n",
      "\\2020-04-29.json\n",
      "DONE\n",
      "\\2020-04-30.json\n",
      "DONE\n",
      "\\2020-05-01.json\n",
      "DONE\n",
      "\\2020-05-02.json\n",
      "DONE\n",
      "\\2020-05-03.json\n",
      "DONE\n",
      "\\2020-05-04.json\n",
      "DONE\n",
      "\\2020-05-05.json\n",
      "DONE\n",
      "\\2020-05-06.json\n",
      "DONE\n",
      "\\2020-05-07.json\n",
      "DONE\n",
      "\\2020-05-08.json\n",
      "DONE\n",
      "\\2020-05-09.json\n",
      "DONE\n",
      "\\2020-05-10.json\n",
      "DONE\n",
      "\\2020-05-11.json\n",
      "DONE\n",
      "\\2020-05-12.json\n",
      "DONE\n",
      "\\2020-05-13.json\n",
      "DONE\n",
      "\\2020-05-14.json\n",
      "DONE\n",
      "\\2020-05-15.json\n",
      "DONE\n",
      "\\2020-05-16.json\n",
      "DONE\n",
      "\\2020-05-17.json\n",
      "DONE\n",
      "\\2020-05-18.json\n",
      "DONE\n",
      "\\2020-05-19.json\n",
      "DONE\n",
      "\\2020-05-20.json\n",
      "DONE\n",
      "\\2020-05-21.json\n",
      "DONE\n",
      "\\2020-05-22.json\n",
      "DONE\n",
      "\\2020-05-23.json\n",
      "DONE\n",
      "\\2020-05-24.json\n",
      "DONE\n",
      "\\2020-05-25.json\n",
      "DONE\n",
      "\\2020-05-26.json\n",
      "DONE\n",
      "\\2020-05-27.json\n",
      "DONE\n",
      "\\2020-05-28.json\n",
      "DONE\n",
      "\\2020-05-29.json\n",
      "DONE\n",
      "\\2020-05-30.json\n",
      "DONE\n",
      "\\2020-05-31.json\n",
      "DONE\n",
      "\\2020-06-01.json\n",
      "DONE\n",
      "\\2020-06-02.json\n",
      "DONE\n",
      "\\2020-06-03.json\n",
      "DONE\n",
      "\\2020-06-04.json\n",
      "DONE\n",
      "\\2020-06-05.json\n",
      "DONE\n",
      "\\2020-06-06.json\n",
      "DONE\n",
      "\\2020-06-07.json\n",
      "DONE\n",
      "\\2020-06-08.json\n",
      "DONE\n",
      "\\2020-06-09.json\n",
      "DONE\n",
      "\\2020-06-10.json\n",
      "DONE\n",
      "\\2020-06-11.json\n",
      "DONE\n",
      "\\2020-06-12.json\n",
      "DONE\n",
      "\\2020-06-13.json\n",
      "DONE\n",
      "\\2020-06-14.json\n",
      "DONE\n",
      "\\2020-06-15.json\n",
      "DONE\n",
      "\\2020-06-16.json\n",
      "DONE\n",
      "\\2020-06-17.json\n",
      "DONE\n",
      "\\2020-06-18.json\n",
      "DONE\n",
      "\\2020-06-19.json\n",
      "DONE\n",
      "\\2020-06-20.json\n",
      "DONE\n",
      "\\2020-06-21.json\n",
      "DONE\n",
      "\\2020-06-22.json\n",
      "DONE\n",
      "\\2020-06-23.json\n",
      "DONE\n",
      "\\2020-06-24.json\n",
      "DONE\n",
      "\\2020-06-25.json\n",
      "DONE\n",
      "\\2020-06-26.json\n",
      "DONE\n",
      "\\2020-06-27.json\n",
      "DONE\n",
      "\\2020-06-28.json\n",
      "DONE\n",
      "\\2020-06-29.json\n",
      "DONE\n",
      "\\2020-06-30.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "all_count = 0\n",
    "tweet_count=0\n",
    "pronoun_tweet_count = 0\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            all_count+=1\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                if \"extended_tweet\" in data:\n",
    "                    text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                    tweet_count+=1\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweet_count += 1\n",
    "                else:\n",
    "                    text = data[\"text\"]\n",
    "                    tweet_count+=1\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweet_count += 1                        \n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202757 116222 37129\n"
     ]
    }
   ],
   "source": [
    "print(all_count, tweet_count, pronoun_tweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "if pronouns == True:\n",
    "    additional_dir = 'я-pronoun.ftr'\n",
    "else:\n",
    "    additional_dir = '-without-мне-pronouns.ftr'\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print('../ftrs/ind-retweets/'+ file[8:])\n",
    "    rows = []\n",
    "    links = []\n",
    "    count=0\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to save data to\n",
    "        save_dir = '../ftrs/ind-retweets/'+ file[8:].replace('.json', additional_dir)\n",
    "        print(save_dir)\n",
    "\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            #username\n",
    "            user_id = data[\"user\"][\"screen_name\"]\n",
    "            #location\n",
    "            user_location = data[\"user\"][\"location\"]\n",
    "            #Tweet ID\n",
    "            tweet_id = data[\"id_str\"]\n",
    "            #Hashtags\n",
    "            hashtags = data[\"entities\"][\"hashtags\"]\n",
    "            # Retweet?\n",
    "            retweet = data[\"retweeted\"]\n",
    "            #tweet text\n",
    "            if \"extended_tweet\" in data:\n",
    "                text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                if pronoun_checker(text) == pronouns:\n",
    "                    rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "            else:\n",
    "                text = data[\"text\"]\n",
    "                if pronoun_checker(text) == pronouns:\n",
    "                    rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "                            \n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\", \"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\", save_dir)\n",
    "\n",
    "display(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\2020-03-01.json\n",
      "DONE\n",
      "\\2020-03-02.json\n",
      "DONE\n",
      "\\2020-03-03.json\n",
      "DONE\n",
      "\\2020-03-04.json\n",
      "DONE\n",
      "\\2020-03-05.json\n",
      "DONE\n",
      "\\2020-03-06.json\n",
      "DONE\n",
      "\\2020-03-07.json\n",
      "DONE\n",
      "\\2020-03-08.json\n",
      "DONE\n",
      "\\2020-03-09.json\n",
      "DONE\n",
      "\\2020-03-10.json\n",
      "DONE\n",
      "\\2020-03-11.json\n",
      "DONE\n",
      "\\2020-03-12.json\n",
      "DONE\n",
      "\\2020-03-13.json\n",
      "DONE\n",
      "\\2020-03-14.json\n",
      "DONE\n",
      "\\2020-03-15.json\n",
      "DONE\n",
      "\\2020-03-16.json\n",
      "DONE\n",
      "\\2020-03-17.json\n",
      "DONE\n",
      "\\2020-03-18.json\n",
      "DONE\n",
      "\\2020-03-19.json\n",
      "DONE\n",
      "\\2020-03-20.json\n",
      "DONE\n",
      "\\2020-03-21.json\n",
      "DONE\n",
      "\\2020-03-22.json\n",
      "DONE\n",
      "\\2020-03-23.json\n",
      "DONE\n",
      "\\2020-03-24.json\n",
      "DONE\n",
      "\\2020-03-25.json\n",
      "DONE\n",
      "\\2020-03-26.json\n",
      "DONE\n",
      "\\2020-03-27.json\n",
      "DONE\n",
      "\\2020-03-28.json\n",
      "DONE\n",
      "\\2020-03-29.json\n",
      "DONE\n",
      "\\2020-03-30.json\n",
      "DONE\n",
      "\\2020-03-31.json\n",
      "DONE\n",
      "\\2020-04-01.json\n",
      "DONE\n",
      "\\2020-04-02.json\n",
      "DONE\n",
      "\\2020-04-03.json\n",
      "DONE\n",
      "\\2020-04-04.json\n",
      "DONE\n",
      "\\2020-04-05.json\n",
      "DONE\n",
      "\\2020-04-06.json\n",
      "DONE\n",
      "\\2020-04-07.json\n",
      "DONE\n",
      "\\2020-04-08.json\n",
      "DONE\n",
      "\\2020-04-09.json\n",
      "DONE\n",
      "\\2020-04-10.json\n",
      "DONE\n",
      "\\2020-04-11.json\n",
      "DONE\n",
      "\\2020-04-12.json\n",
      "DONE\n",
      "\\2020-04-13.json\n",
      "DONE\n",
      "\\2020-04-14.json\n",
      "DONE\n",
      "\\2020-04-15.json\n",
      "DONE\n",
      "\\2020-04-16.json\n",
      "DONE\n",
      "\\2020-04-17.json\n",
      "DONE\n",
      "\\2020-04-18.json\n",
      "DONE\n",
      "\\2020-04-19.json\n",
      "DONE\n",
      "\\2020-04-20.json\n",
      "DONE\n",
      "\\2020-04-21.json\n",
      "DONE\n",
      "\\2020-04-22.json\n",
      "DONE\n",
      "\\2020-04-23.json\n",
      "DONE\n",
      "\\2020-04-24.json\n",
      "DONE\n",
      "\\2020-04-25.json\n",
      "DONE\n",
      "\\2020-04-26.json\n",
      "DONE\n",
      "\\2020-04-27.json\n",
      "DONE\n",
      "\\2020-04-28.json\n",
      "DONE\n",
      "\\2020-04-29.json\n",
      "DONE\n",
      "\\2020-04-30.json\n",
      "DONE\n",
      "\\2020-05-01.json\n",
      "DONE\n",
      "\\2020-05-02.json\n",
      "DONE\n",
      "\\2020-05-03.json\n",
      "DONE\n",
      "\\2020-05-04.json\n",
      "DONE\n",
      "\\2020-05-05.json\n",
      "DONE\n",
      "\\2020-05-06.json\n",
      "DONE\n",
      "\\2020-05-07.json\n",
      "DONE\n",
      "\\2020-05-08.json\n",
      "DONE\n",
      "\\2020-05-09.json\n",
      "DONE\n",
      "\\2020-05-10.json\n",
      "DONE\n",
      "\\2020-05-11.json\n",
      "DONE\n",
      "\\2020-05-12.json\n",
      "DONE\n",
      "\\2020-05-13.json\n",
      "DONE\n",
      "\\2020-05-14.json\n",
      "DONE\n",
      "\\2020-05-15.json\n",
      "DONE\n",
      "\\2020-05-16.json\n",
      "DONE\n",
      "\\2020-05-17.json\n",
      "DONE\n",
      "\\2020-05-18.json\n",
      "DONE\n",
      "\\2020-05-19.json\n",
      "DONE\n",
      "\\2020-05-20.json\n",
      "DONE\n",
      "\\2020-05-21.json\n",
      "DONE\n",
      "\\2020-05-22.json\n",
      "DONE\n",
      "\\2020-05-23.json\n",
      "DONE\n",
      "\\2020-05-24.json\n",
      "DONE\n",
      "\\2020-05-25.json\n",
      "DONE\n",
      "\\2020-05-26.json\n",
      "DONE\n",
      "\\2020-05-27.json\n",
      "DONE\n",
      "\\2020-05-28.json\n",
      "DONE\n",
      "\\2020-05-29.json\n",
      "DONE\n",
      "\\2020-05-30.json\n",
      "DONE\n",
      "\\2020-05-31.json\n",
      "DONE\n",
      "\\2020-06-01.json\n",
      "DONE\n",
      "\\2020-06-02.json\n",
      "DONE\n",
      "\\2020-06-03.json\n",
      "DONE\n",
      "\\2020-06-04.json\n",
      "DONE\n",
      "\\2020-06-05.json\n",
      "DONE\n",
      "\\2020-06-06.json\n",
      "DONE\n",
      "\\2020-06-07.json\n",
      "DONE\n",
      "\\2020-06-08.json\n",
      "DONE\n",
      "\\2020-06-09.json\n",
      "DONE\n",
      "\\2020-06-10.json\n",
      "DONE\n",
      "\\2020-06-11.json\n",
      "DONE\n",
      "\\2020-06-12.json\n",
      "DONE\n",
      "\\2020-06-13.json\n",
      "DONE\n",
      "\\2020-06-14.json\n",
      "DONE\n",
      "\\2020-06-15.json\n",
      "DONE\n",
      "\\2020-06-16.json\n",
      "DONE\n",
      "\\2020-06-17.json\n",
      "DONE\n",
      "\\2020-06-18.json\n",
      "DONE\n",
      "\\2020-06-19.json\n",
      "DONE\n",
      "\\2020-06-20.json\n",
      "DONE\n",
      "\\2020-06-21.json\n",
      "DONE\n",
      "\\2020-06-22.json\n",
      "DONE\n",
      "\\2020-06-23.json\n",
      "DONE\n",
      "\\2020-06-24.json\n",
      "DONE\n",
      "\\2020-06-25.json\n",
      "DONE\n",
      "\\2020-06-26.json\n",
      "DONE\n",
      "\\2020-06-27.json\n",
      "DONE\n",
      "\\2020-06-28.json\n",
      "DONE\n",
      "\\2020-06-29.json\n",
      "DONE\n",
      "\\2020-06-30.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "all_tweets_word_count = 0\n",
    "all_tweets = 0\n",
    "pronoun_tweet_word_count = 0\n",
    "pronoun_tweets = 0\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                if \"extended_tweet\" in data:\n",
    "                    text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweets+=1\n",
    "                        for i in text:\n",
    "                            pronoun_tweet_word_count += 1\n",
    "                            \n",
    "                    else:\n",
    "                        all_tweets+=1\n",
    "                        for i in text:\n",
    "                            all_tweets_word_count+=1\n",
    "                            \n",
    "                else:\n",
    "                    text = data[\"text\"]\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweets+=1\n",
    "                        for i in text:\n",
    "                            pronoun_tweet_word_count += 1\n",
    "                    else:\n",
    "                        all_tweets+=1\n",
    "                        for i in text:\n",
    "                            all_tweets_word_count+=1\n",
    "                            \n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79093\n",
      "37129\n",
      "142.13956987344014\n",
      "193.11368472083817\n"
     ]
    }
   ],
   "source": [
    "#202757 116222 37129\n",
    "print(all_tweets)\n",
    "print(pronoun_tweets)\n",
    "print(all_tweets_word_count/all_tweets)\n",
    "print(pronoun_tweet_word_count/pronoun_tweets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
