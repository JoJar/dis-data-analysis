{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, nltk\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "from nltk import bigrams, ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = ['а', 'в', 'г', 'е', 'ж', 'и', 'к', 'м', 'о', 'с', 'т', 'у', 'я', 'бы', 'во', 'вы', 'да', 'до', 'ее', 'ей', 'ею', 'её', 'же', 'за', 'из', 'им', 'их', 'ли', 'мы', 'на', 'не', 'ни', 'но', 'ну', 'нх', 'об', 'он', 'от', 'по', 'со', 'та', 'те', 'то', 'ту', 'ты', 'уж', 'без', 'был', 'вам', 'вас', 'ваш', 'вон', 'вот', 'все', 'всю', 'вся', 'всё', 'где', 'год', 'два', 'две', 'дел', 'для', 'его', 'ему', 'еще', 'ещё', 'или', 'ими', 'имя', 'как', 'кем', 'ком', 'кто', 'лет', 'мне', 'мог', 'мож', 'мои', 'мой', 'мор', 'моя', 'моё', 'над', 'нам', 'нас', 'наш', 'нее', 'ней', 'нем', 'нет', 'нею', 'неё', 'них', 'оба', 'она', 'они', 'оно', 'под', 'пор', 'при', 'про', 'раз', 'сам', 'сих', 'так', 'там', 'тем', 'тех', 'том', 'тот', 'тою', 'три', 'тут', 'уже', 'чем', 'что', 'эта', 'эти', 'это', 'эту', 'алло', 'буду', 'будь', 'бывь', 'была', 'были', 'было', 'быть', 'вами', 'ваша', 'ваше', 'ваши', 'ведь', 'весь', 'вниз', 'всем', 'всех', 'всею', 'года', 'году', 'даже', 'двух', 'день', 'если', 'есть', 'зато', 'кого', 'кому', 'куда', 'лишь', 'люди', 'мало', 'меля', 'меня', 'мимо', 'мира', 'мной', 'мною', 'мочь', 'надо', 'нами', 'наша', 'наше', 'наши', 'него', 'нему', 'ниже', 'ними', 'один', 'пока', 'пора', 'пять', 'рано', 'сама', 'сами', 'само', 'саму', 'свое', 'свои', 'свою', 'себе', 'себя', 'семь', 'стал', 'суть', 'твой', 'твоя', 'твоё', 'тебе', 'тебя', 'теми', 'того', 'тоже', 'тому', 'туда', 'хоть', 'хотя', 'чаще', 'чего', 'чему', 'чтоб', 'чуть', 'этим', 'этих', 'этой', 'этом', 'этот', 'более', 'будем', 'будет', 'будто', 'будут', 'вверх', 'вдали', 'вдруг', 'везде', 'внизу', 'время', 'всего', 'всеми', 'всему', 'всюду', 'давно', 'даром', 'долго', 'друго', 'жизнь', 'занят', 'затем', 'зачем', 'здесь', 'иметь', 'какая', 'какой', 'когда', 'кроме', 'лучше', 'между', 'менее', 'много', 'могут', 'может', 'можно', 'можхо', 'назад', 'низко', 'нужно', 'одной', 'около', 'опять', 'очень', 'перед', 'позже', 'после', 'потом', 'почти', 'пятый', 'разве', 'рядом', 'самим', 'самих', 'самой', 'самом', 'своей', 'своих', 'сеаой', 'снова', 'собой', 'собою', 'такая', 'также', 'такие', 'такое', 'такой', 'тобой', 'тобою', 'тогда', 'тысяч', 'уметь', 'часто', 'через', 'чтобы', 'шесть', 'этими', 'этого', 'этому', 'близко', 'больше', 'будете', 'будешь', 'бывает', 'важная', 'важное', 'важные', 'важный', 'вокруг', 'восемь', 'всегда', 'второй', 'далеко', 'дальше', 'девять', 'десять', 'должно', 'другая', 'другие', 'других', 'другое', 'другой', 'занята', 'занято', 'заняты', 'значит', 'именно', 'иногда', 'каждая', 'каждое', 'каждые', 'каждый', 'кругом', 'меньше', 'начала', 'нельзя', 'нибудь', 'никуда', 'ничего', 'обычно', 'однако', 'одного', 'отсюда', 'первый', 'потому', 'почему', 'просто', 'против', 'раньше', 'самими', 'самого', 'самому', 'своего', 'сейчас', 'сказал', 'совсем', 'теперь', 'только', 'третий', 'хорошо', 'хотеть', 'хочешь', 'четыре', 'шестой', 'восьмой', 'впрочем', 'времени', 'говорил', 'говорит', 'девятый', 'десятый', 'кажется', 'конечно', 'которая', 'которой', 'которые', 'который', 'которых', 'наверху', 'наконец', 'недавно', 'немного', 'нередко', 'никогда', 'однажды', 'посреди', 'сегодня', 'седьмой', 'сказала', 'сказать', 'сколько', 'слишком', 'сначала', 'спасибо', 'человек', 'двадцать', 'довольно', 'которого', 'наиболее', 'недалеко', 'особенно', 'отовсюду', 'двадцатый', 'миллионов', 'несколько', 'прекрасно', 'процентов', 'четвертый', 'двенадцать', 'непрерывно', 'пожалуйста', 'пятнадцать', 'семнадцать', 'тринадцать', 'двенадцатый', 'одиннадцать', 'пятнадцатый', 'семнадцатый', 'тринадцатый', 'шестнадцать', 'восемнадцать', 'девятнадцать', 'одиннадцатый', 'четырнадцать', 'шестнадцатый', 'восемнадцатый', 'девятнадцатый', 'действительно', 'четырнадцатый', 'многочисленная', 'многочисленное', 'многочисленные', 'многочисленный', \"я\", \"мы\", \"ты\", \"oни\", \"нас\", \"вы\", \"тебя\", \"меня\", \"тебе\", \"вами\", \"мне\", \"тысяч\", \"котор\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(n, path, filter_word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lt = [x[0] for x in os.walk(path)]\n",
    "\n",
    "    dict_pronoun_words = {}\n",
    "    bigrams_dict = {}\n",
    "\n",
    "    for ftr_dir in lt[1:]:\n",
    "        # Gets pronoun from file name\n",
    "        pronoun = ftr_dir[17:]\n",
    "\n",
    "        # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "        ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "\n",
    "        file_list = glob.glob(ftr_pattern)\n",
    "        \n",
    "        for i in file_list:\n",
    "            bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "            flat_list = []\n",
    "            read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "            ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "            flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "            dict_pronoun_words[pronoun] = flat_list\n",
    "        \n",
    "        if n == 2:\n",
    "            # Bigrams\n",
    "            for i in dict_pronoun_words:\n",
    "                tokens = dict_pronoun_words.get(i)\n",
    "                tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "                tokens= tokens.split()\n",
    "                tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "                finder = BigramCollocationFinder.from_words(tokens)\n",
    "                # only bigrams that appear 3+ times\n",
    "                finder.apply_freq_filter(1)\n",
    "                # Ngrams with 'creature' as a member\n",
    "                ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "                # only bigrams that contain the filter word\n",
    "                finder.apply_ngram_filter(ngram_word_filter)\n",
    "                #text_bigrams = bigrams(tokens)\n",
    "                bigrams_dict[i] = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "                print(bigrams_dict.get(i))\n",
    "        \n",
    "        if n == 3:\n",
    "            for i in dict_pronoun_words:\n",
    "                tokens = dict_pronoun_words.get(i)\n",
    "                tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "                tokens= tokens.split()\n",
    "                tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "                finder = TrigramCollocationFinder.from_words(tokens)\n",
    "                # only bigrams that appear 3+ times\n",
    "                finder.apply_freq_filter(1)\n",
    "                # Ngrams with 'creature' as a member\n",
    "                ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "                # only bigrams that contain the filter word\n",
    "                finder.apply_ngram_filter(ngram_word_filter)\n",
    "                #text_bigrams = bigrams(tokens)\n",
    "                bigrams_dict[i] = finder.nbest(trigram_measures.likelihood_ratio, 10)\n",
    "\n",
    "    if n <= 3:\n",
    "        return bigrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_2(n, path, filter_word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    dict_pronoun_words = {}\n",
    "    bigrams_dict = {}\n",
    "\n",
    "    # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "    ftr_dir = '../ftrs/individual-ftrs/'\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    flat_list = []\n",
    "    for i in file_list:\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "        flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "    \n",
    "    if n == 2:\n",
    "        # Bigrams\n",
    "        for i in dict_pronoun_words:\n",
    "            tokens = dict_pronoun_words.get(i)\n",
    "            tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "            tokens= tokens.split()\n",
    "            tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "            # only bigrams that appear 3+ times\n",
    "            finder.apply_freq_filter(1)\n",
    "            # Ngrams with 'creature' as a member\n",
    "            ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "            # only bigrams that contain the filter word\n",
    "            finder.apply_ngram_filter(ngram_word_filter)\n",
    "            #text_bigrams = bigrams(tokens)\n",
    "            bigrams_dict[i] = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "    \n",
    "    if n == 3:\n",
    "        flat_list = [wrd for wrd in flat_list if wrd.lower() not in stop_words]\n",
    "        finder = TrigramCollocationFinder.from_words(flat_list)\n",
    "        # only bigrams that appear 3+ times\n",
    "        finder.apply_freq_filter(5)\n",
    "        # Ngrams with 'creature' as a member\n",
    "        ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "        # only bigrams that contain the filter word\n",
    "        finder.apply_ngram_filter(ngram_word_filter)\n",
    "        #text_bigrams = bigrams(tokens)\n",
    "        trigrams = finder.nbest(trigram_measures.likelihood_ratio, 10)\n",
    "\n",
    "    if n <= 3:\n",
    "        return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_знает = get_ngrams(3, '../ftrs/Pronouns/', 'знает')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oни \n",
      " [('galinaotradnoe', 'знает', 'ге'), ('mpltnv', 'galinaotradnoe', 'знает'), ('знает', 'ге', 'долж')]\n",
      "вас \n",
      " [('mpltnv', 'galinaotradnoe', 'знает'), ('движк', 'ху', 'знает'), ('морожен', 'реж', 'знает'), ('знает', 'разыгрыва', 'чудесн'), ('galinaotradnoe', 'знает', 'ге'), ('знает', 'вейб', 'мног'), ('знает', 'масс', 'kovalyash'), ('общ', 'знает', 'вейб'), ('руководств', 'лгу', 'знает'), ('откуд', 'поч', 'знает')]\n",
      "вы \n",
      " [('mpltnv', 'galinaotradnoe', 'знает'), ('морожен', 'реж', 'знает'), ('движк', 'ху', 'знает'), ('знает', 'бренд', 'перекрашива'), ('знает', 'разыгрыва', 'чудесн'), ('galinaotradnoe', 'знает', 'ге'), ('знает', 'вейб', 'мног'), ('знает', 'масс', 'wlodzislavs'), ('реж', 'знает', 'масс'), ('откуд', 'поч', 'знает')]\n",
      "их \n",
      " [('движк', 'ху', 'знает'), ('знает', 'разыгрыва', 'чудесн'), ('знает', 'вейб', 'мног'), ('общ', 'знает', 'вейб'), ('помн', 'лгу', 'знает'), ('знает', 'реж', 'са'), ('ху', 'знает', 'реж'), ('дела', 'филарет', 'знает'), ('филарет', 'знает', 'дела'), ('откуд', 'поч', 'знает')]\n",
      "меня \n",
      " [('mpltnv', 'galinaotradnoe', 'знает'), ('знает', 'разыгрыва', 'чудесн'), ('област', 'лгу', 'знает'), ('galinaotradnoe', 'знает', 'ге'), ('уточня', 'дела', 'знает'), ('знает', 'ге', 'долж'), ('знает', 'писа', 'заяв'), ('эт', 'знает', 'разыгрыва'), ('дела', 'знает', 'писа'), ('знает', 'эт', 'вообщ')]\n",
      "мне \n",
      " []\n",
      "мы \n",
      " [('знает', 'разыгрыва', 'чудесн'), ('помн', 'лгу', 'знает'), ('эт', 'знает', 'разыгрыва'), ('знает', 'эт', 'вообщ'), ('лгу', 'знает', 'эт'), ('люд', 'эт', 'знает')]\n",
      "нас \n",
      " [('mpltnv', 'galinaotradnoe', 'знает'), ('движк', 'ху', 'знает'), ('знает', 'реж', 'са'), ('galinaotradnoe', 'знает', 'ге'), ('ху', 'знает', 'реж'), ('знает', 'ге', 'долж')]\n",
      "тебя \n",
      " [('движк', 'ху', 'знает'), ('ху', 'знает', 'немн'), ('знает', 'немн', 'буд')]\n",
      "ты \n",
      " [('движк', 'ху', 'знает'), ('ху', 'знает', 'немн'), ('знает', 'немн', 'буд')]\n",
      "я \n",
      " [('mpltnv', 'galinaotradnoe', 'знает'), ('знает', 'разыгрыва', 'чудесн'), ('област', 'лгу', 'знает'), ('galinaotradnoe', 'знает', 'ге'), ('уточня', 'дела', 'знает'), ('знает', 'ге', 'долж'), ('знает', 'писа', 'заяв'), ('эт', 'знает', 'разыгрыва'), ('дела', 'знает', 'писа'), ('лгу', 'знает', 'эт')]\n"
     ]
    }
   ],
   "source": [
    "for i in trigrams_dic_знает:\n",
    "    print(i, \"\\n\", trigrams_dic_знает.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('эфир', 'шлок', 'знает'), ('знает', 'домдв', 'домдв'), ('знает', 'нефтян', 'кризис'), ('раков', 'клеток', 'знает'), ('знает', 'via', 'youtube'), ('via', 'youtube', 'знает'), ('знает', 'друг', 'друг'), ('друг', 'друг', 'знает'), ('ал', 'савкин', 'знает'), ('семьдес', 'сем', 'знает')]\n"
     ]
    }
   ],
   "source": [
    "# for i in trigrams_dic_знает:\n",
    "#     print(i, \"\\n\", trigrams_dic_знает.get(i))\n",
    "print(trigrams_dic_знает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_тоже = get_ngrams(3, '../ftrs/Pronouns/', 'тоже')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oни \n",
      " []\n",
      "вас \n",
      " []\n",
      "вы \n",
      " []\n",
      "их \n",
      " []\n",
      "меня \n",
      " []\n",
      "мне \n",
      " []\n",
      "мы \n",
      " []\n",
      "нас \n",
      " []\n",
      "тебя \n",
      " []\n",
      "ты \n",
      " []\n",
      "я \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "for i in trigrams_dic_тоже:\n",
    "    print(i, \"\\n\", trigrams_dic_тоже.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rt_bigrams_dic = get_ngrams(2, '../ftrs/Pronouns/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bigrams_dic:\n",
    "    print(i, \"\\n\", no_rt_bigrams_dic.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [('зомбиящик', 'крым'), ('коммент', 'естествен'), ('крым', 'фашист'), ('ленин', 'почтамт'), ('прощен', 'коммент'), ('фашист', 'цт'), ('плачут', 'прос'), ('почтамт', 'телефон'), ('прос', 'прощен'), ('телефон', 'телегаф')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_bigrams_dic = get_ngrams(2, '../ftrs/ind-retweets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ets/I \n",
      " [('вестпойнт', 'лозунг'), ('лгат', 'краст'), ('лозунг', 'лгат'), ('помпе', 'уч'), ('уч', 'вестпойнт'), ('alexeypushkov', 'помпе'), ('краст', 'мир'), ('rt', 'alexeypushkov'), ('мир', 'эт'), ('оставля', 'отзыв')]\n"
     ]
    }
   ],
   "source": [
    "for i in rt_bigrams_dic:\n",
    "    print(i, \"\\n\", rt_bigrams_dic.get(i))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
