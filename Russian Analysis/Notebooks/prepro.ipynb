{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import nltk\r\n",
    "from nltk.stem import SnowballStemmer\r\n",
    "import re\r\n",
    "from pymystem3 import Mystem\r\n",
    "from num2words import num2words\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import plotly.graph_objects as go"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def preprocess(text):\r\n",
    "    text = text.lower()\r\n",
    "    text = remove_apostrophes(text)\r\n",
    "    text = remove_punctuation(text)\r\n",
    "    text = remove_single_chars(text)\r\n",
    "    text = remove_search_terms(text)\r\n",
    "    text = remove_links(text)\r\n",
    "    text = numbers_to_words(text)\r\n",
    "\r\n",
    "    text = tokenize(text)\r\n",
    "    text = remove_stop_words(text)\r\n",
    "    text = stemmer(text)\r\n",
    "\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def stemmer(text):\r\n",
    "    #the stemmer requires a language parameter\r\n",
    "    snow_stemmer = SnowballStemmer(language='russian')\r\n",
    "    # mystem = Mystem()\r\n",
    "    # tokens = mystem.lemmatize(text)\r\n",
    "\r\n",
    "    stem_words = []\r\n",
    "    for w in text.split():\r\n",
    "        x = snow_stemmer.stem(w)\r\n",
    "        stem_words.append(x)\r\n",
    "    return stem_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from stop_words import get_stop_words\r\n",
    "\r\n",
    "#stop_words = get_stop_words('ru')\r\n",
    "\r\n",
    "def remove_stop_words(text):\r\n",
    "    vocab = [\"я\", \"мы\", \"ты\", \"нас\", \"вы\", \"тебя\", \"меня\",\"мне\", \"вас\", \"их\", \"они\"]\r\n",
    "    russian_stopwords = stopwords.words(\"russian\")\r\n",
    "\r\n",
    "    stop_words = [word for word in russian_stopwords if word not in vocab]\r\n",
    "    \r\n",
    "    filtered_words = [word for word in text if word not in stop_words]\r\n",
    "    text = \" \".join(filtered_words)\r\n",
    "    return filtered_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def numbers_to_words(text):\r\n",
    "    num_list = re.findall(r'\\d+', text)\r\n",
    "    str_num_list = []\r\n",
    "\r\n",
    "    for num in num_list:\r\n",
    "            str_with_punct = num2words(num, lang='ru')\r\n",
    "            str_without_punct = re.sub('[^а-яА-Я0-9 ]+', '', str_with_punct)\r\n",
    "            \r\n",
    "            str_num_list.append(str_without_punct)\r\n",
    "    \r\n",
    "    for i, num in enumerate(num_list):\r\n",
    "        text = text.replace(str(num), str_num_list[i])\r\n",
    "  \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def remove_single_chars(text):\r\n",
    "    txt = text.split()\r\n",
    "    updated_text = \"\"\r\n",
    "    for word in txt:\r\n",
    "        if len(word) > 1 or word.lower() == \"я\":\r\n",
    "            updated_text = updated_text + \" \" + word\r\n",
    "    return updated_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Remove apostrophes\r\n",
    "def remove_apostrophes(text):\r\n",
    "    text = text.replace(\"'\", \"\")\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Remove Links\r\n",
    "def remove_links(text):\r\n",
    "    text = re.sub(r'http\\S+', '', text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Tokenizes words in document text\r\n",
    "def tokenize(text):\r\n",
    "    tokens = nltk.word_tokenize(text)\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def remove_punctuation(text):\r\n",
    "    words = []\r\n",
    "    for word in text:\r\n",
    "        w = re.sub(r'([^\\w\\s\\d]|_)','',word)\r\n",
    "        words.append(w)\r\n",
    "    words = \"\".join(words)\r\n",
    "\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Stems tokenized words\r\n",
    "def stemmer(text):\r\n",
    "    stemmer = SnowballStemmer(\"russian\")\r\n",
    "    stems = []\r\n",
    "\r\n",
    "    for item in text:\r\n",
    "        # Uses SnowBall stemmer as it is compatible with Russian.\r\n",
    "        # This strips prefixes/suffixes to give a base word.\r\n",
    "        stems.append(stemmer.stem(item))\r\n",
    "    return stems"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Remove Search Terms\r\n",
    "def remove_search_terms(stripped_string):\r\n",
    "    stripped_string = stripped_string.replace(\"фейки\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"пропаганда\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"дезинформация\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"слухи\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"утки\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"вранье\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"активные мероприятия\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"диверсия\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"вмешательство\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"влияние\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"конспирология\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"Глубинное государство\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"фабрикация\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"манипулировать\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"ложный\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"мошенничество\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"обманывать\", \"\").strip()\r\n",
    "    return stripped_string"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}