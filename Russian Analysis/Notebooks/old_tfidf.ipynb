{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import nltk, os, glob\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from nltk.stem.porter import PorterStemmer\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "from num2words import num2words\r\n",
    "import plotly.graph_objects as go\r\n",
    "import import_ipynb\r\n",
    "\r\n",
    "nltk.download('averaged_perceptron_tagger_ru')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     C:\\Users\\c1635922\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Function to take file path and return path object which will get all files of that type in the directory.\r\n",
    "# Takes two param: filename and filetype.\r\n",
    "def file_to_path(file_dir, filetype):\r\n",
    "    # Finds all files of the specified type in the directory\r\n",
    "    txt_path = os.path.join(file_dir, '*.' + filetype)\r\n",
    "\r\n",
    "    # Uses Glob with the txt_path variable to place all the file directories in a list\r\n",
    "    file_list = glob.glob(txt_path)\r\n",
    "\r\n",
    "    return file_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Function to create corpus\r\n",
    "def create_corpus(path):\r\n",
    "    token_corpus = {}\r\n",
    "    for f in path:\r\n",
    "        with open(f, 'r', encoding=\"utf-8\") as fs:\r\n",
    "                text = fs.read()\r\n",
    "                text = text.replace('\\n', ' ')\r\n",
    "                #text = prepro.preprocess(text)\r\n",
    "                token_corpus[f] = text.replace('\\n', ' ')\r\n",
    "    return token_corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Function to create comparison file\r\n",
    "def create_comparison_str(path):\r\n",
    "    token_file = \"\"\r\n",
    "\r\n",
    "    for f in path:\r\n",
    "        with open(f, 'r', encoding=\"utf-8\") as fs:\r\n",
    "            text = fs.read()\r\n",
    "            text = text.replace('\\n', ' ')\r\n",
    "            #text = prepro.preprocess(text)\r\n",
    "            token_file += text\r\n",
    "    return token_file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "corpus_files = file_to_path('../txts/individual-days/', 'txt')\r\n",
    "token_corpus = create_corpus(corpus_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "comparison_files = file_to_path('../txts/Pronoun/', 'txt')\r\n",
    "token_file = create_comparison_str(comparison_files)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# tranform pronoun file\r\n",
    "def transform_each_pronoun_doc():\r\n",
    "    all_pronoun_files_and_text = {}\r\n",
    "    queries = [\"я\", \"мы\", \"ты\", \"нас\", \"вы\", \"тебя\", \"меня\", \"тебе\", \"мне\"]\r\n",
    "\r\n",
    "    for query in queries:\r\n",
    "        all_pronoun_files_and_text[query] = save_file_string(query)\r\n",
    "\r\n",
    "    return all_pronoun_files_and_text\r\n",
    "\r\n",
    "def save_file_string(query):\r\n",
    "    base_path = '../txts/Pronoun/' + query\r\n",
    "    path_to_files = file_to_path(base_path, 'txt')\r\n",
    "    return create_comparison_str(path_to_files)\r\n",
    "\r\n",
    "pronoun_dictionary = transform_each_pronoun_doc()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "token_я = pronoun_dictionary.get(\"я\")\r\n",
    "token_мы = pronoun_dictionary.get(\"мы\")\r\n",
    "token_ты = pronoun_dictionary.get(\"ты\")\r\n",
    "token_нас = pronoun_dictionary.get(\"нас\")\r\n",
    "token_вы = pronoun_dictionary.get(\"вы\")\r\n",
    "token_тебя = pronoun_dictionary.get(\"тебя\")\r\n",
    "token_меня = pronoun_dictionary.get(\"меня\")\r\n",
    "token_тебе = pronoun_dictionary.get(\"тебе\")\r\n",
    "token_мне = pronoun_dictionary.get(\"мне\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# https://towardsdatascience.com/higher-accuracy-and-less-process-time-in-text-classification-with-lda-and-tf-idf-d2d949e344c3\r\n",
    "# https://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\r\n",
    "# https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\r\n",
    "\r\n",
    "# The transform() function computes the tfidf frequency of each word in the bag of word.\r\n",
    "# Now our aim is to compare the document D2 with D1. It means we want to see how many words\r\n",
    "# of D1 match up with D2. Thats why we perform fit_transform() on D1 and then only the transform()\r\n",
    "# function on D2 would apply the bag of words of D1 and count the inverse frequency of tokens in D2.\r\n",
    "# This would give the relative comparison of D1 against D2.\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=8)\r\n",
    "\r\n",
    "tfidf_matrix = tfidf.fit_transform(token_corpus.values())\r\n",
    "feature_names = tfidf.get_feature_names()\r\n",
    "\r\n",
    "response_я = tfidf.transform([token_я])\r\n",
    "response_мы= tfidf.transform([token_мы])\r\n",
    "response_ты = tfidf.transform([token_ты])\r\n",
    "response_нас = tfidf.transform([token_нас])\r\n",
    "response_вы = tfidf.transform([token_вы])\r\n",
    "response_тебя = tfidf.transform([token_тебя])\r\n",
    "response_меня = tfidf.transform([token_меня])\r\n",
    "response_тебе = tfidf.transform([token_тебе])\r\n",
    "response_мне = tfidf.transform([token_мне])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFIDF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# lst = token_мы.split()\r\n",
    "# for i in lst:\r\n",
    "#     if len(i) == 2:# == \"ве\":\r\n",
    "#         print(i)\r\n",
    "\r\n",
    "print(response_я)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 13191)\t0.000881236878422398\n",
      "  (0, 13190)\t0.0002885325190712215\n",
      "  (0, 13189)\t0.0005401655165393814\n",
      "  (0, 13188)\t0.0044265624076819665\n",
      "  (0, 13185)\t0.004593220505972547\n",
      "  (0, 13184)\t0.0013234798385213715\n",
      "  (0, 13183)\t0.0002956447003864589\n",
      "  (0, 13181)\t0.0002885325190712215\n",
      "  (0, 13180)\t0.0012740651631062937\n",
      "  (0, 13179)\t0.0021830114302204117\n",
      "  (0, 13178)\t0.002084932822315402\n",
      "  (0, 13177)\t0.0022451764562953186\n",
      "  (0, 13175)\t0.001556687171907707\n",
      "  (0, 13174)\t0.0007021656730449253\n",
      "  (0, 13171)\t0.0014881821029542846\n",
      "  (0, 13170)\t0.0017291562492789858\n",
      "  (0, 13169)\t0.0023934798290990733\n",
      "  (0, 13168)\t0.0023244573822296926\n",
      "  (0, 13167)\t0.0015037106197026745\n",
      "  (0, 13166)\t0.0005401655165393814\n",
      "  (0, 13165)\t0.001154130076284886\n",
      "  (0, 13164)\t0.0003118448516217685\n",
      "  (0, 13163)\t0.006584284437695411\n",
      "  (0, 13162)\t0.0009355345548653055\n",
      "  (0, 13161)\t0.00913717848686233\n",
      "  :\t:\n",
      "  (0, 55)\t0.009226758668106077\n",
      "  (0, 53)\t0.0006424132850064543\n",
      "  (0, 51)\t0.0006937740022114252\n",
      "  (0, 46)\t0.0005830541585135155\n",
      "  (0, 40)\t0.0008232254341243634\n",
      "  (0, 38)\t0.000837362038586919\n",
      "  (0, 37)\t0.0006067521632812212\n",
      "  (0, 34)\t0.0012168963294190073\n",
      "  (0, 33)\t0.0003118448516217685\n",
      "  (0, 30)\t0.00032120664250322715\n",
      "  (0, 28)\t0.0009408290675707011\n",
      "  (0, 24)\t0.0002885325190712215\n",
      "  (0, 23)\t0.000837362038586919\n",
      "  (0, 22)\t0.0002956447003864589\n",
      "  (0, 19)\t0.0003033760816406106\n",
      "  (0, 18)\t0.00032120664250322715\n",
      "  (0, 17)\t0.0006424132850064543\n",
      "  (0, 13)\t0.0008302535746831144\n",
      "  (0, 12)\t0.00032120664250322715\n",
      "  (0, 10)\t0.0002956447003864589\n",
      "  (0, 8)\t0.0008274519531011589\n",
      "  (0, 7)\t0.0005830541585135155\n",
      "  (0, 2)\t0.000623689703243537\n",
      "  (0, 1)\t0.0004407114185801691\n",
      "  (0, 0)\t0.001435477780434718\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def tfidf_results(item, pronoun):\r\n",
    "    data = {}\r\n",
    "    words = []\r\n",
    "    count = []\r\n",
    "\r\n",
    "    for col in item.nonzero()[1]:\r\n",
    "        if feature_names[col] != \"ве\":\r\n",
    "            words.append(feature_names[col])\r\n",
    "            count.append(item[0, col])\r\n",
    "\r\n",
    "        #word_column = \"Word\" + \" \" + pronouns[i]\r\n",
    "        #tfidf_column = \"TF-IDF\" + \" \" + pronouns[i]\r\n",
    "\r\n",
    "    for x in range(len(words)):\r\n",
    "            data[\"Words-\"+pronoun] = words\r\n",
    "            data[\"TF-IDF-\"+pronoun] = count\r\n",
    "\r\n",
    "    df = pd.DataFrame(data=data)\r\n",
    "    queries = [\"я\", \"мы\", \"ты\", \"нас\", \"вы\", \"тебя\", \"меня\", \"тебе\", \"мне\"]\r\n",
    "\r\n",
    "    df_sorted = df.sort_values(by=[\"TF-IDF-\"+pronoun], ascending=False).reset_index(drop=True)\r\n",
    "    top = df_sorted.head(6)\r\n",
    "    return top.drop([0])\r\n",
    "\r\n",
    "я_pro = tfidf_results(response_я, \"я\")\r\n",
    "мы_pro = tfidf_results(response_мы, \"мы\")\r\n",
    "ты_pro = tfidf_results(response_ты, \"ты\")\r\n",
    "нас_pro = tfidf_results(response_нас, \"нас\")\r\n",
    "вы_pro = tfidf_results(response_вы, \"вы\")\r\n",
    "тебя_pro = tfidf_results(response_тебя, \"тебя\")\r\n",
    "меня_pro = tfidf_results(response_меня, \"меня\")\r\n",
    "тебе_pro = tfidf_results(response_тебе, \"тебе\")\r\n",
    "мне_pro = tfidf_results(response_мне, \"мне\")\r\n",
    "\r\n",
    "result = pd.concat([я_pro, мы_pro, ты_pro, нас_pro, вы_pro, тебя_pro, меня_pro, тебе_pro, мне_pro], axis=1)\r\n",
    "\r\n",
    "column_values = result[[\"Words-я\", \"Words-мы\", \"Words-ты\", \"Words-нас\", \"Words-вы\", \"Words-тебя\",\"Words-меня\", \"Words-тебе\", \"Words-мне\"]].values\r\n",
    "unique_values =  np.unique(column_values)\r\n",
    "\r\n",
    "column_value_count = result[[\"Words-я\", \"Words-мы\", \"Words-ты\", \"Words-нас\", \"Words-вы\", \"Words-тебя\", \"Words-меня\",\"Words-тебе\", \"Words-мне\"]].value_counts()\r\n",
    "unique_values_count =  np.unique(column_value_count)\r\n",
    "\r\n",
    "display(result)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words-я</th>\n",
       "      <th>TF-IDF-я</th>\n",
       "      <th>Words-мы</th>\n",
       "      <th>TF-IDF-мы</th>\n",
       "      <th>Words-ты</th>\n",
       "      <th>TF-IDF-ты</th>\n",
       "      <th>Words-нас</th>\n",
       "      <th>TF-IDF-нас</th>\n",
       "      <th>Words-вы</th>\n",
       "      <th>TF-IDF-вы</th>\n",
       "      <th>Words-тебя</th>\n",
       "      <th>TF-IDF-тебя</th>\n",
       "      <th>Words-меня</th>\n",
       "      <th>TF-IDF-меня</th>\n",
       "      <th>Words-тебе</th>\n",
       "      <th>TF-IDF-тебе</th>\n",
       "      <th>Words-мне</th>\n",
       "      <th>TF-IDF-мне</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>человек</td>\n",
       "      <td>0.232178</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.215112</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.236633</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.201346</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.269101</td>\n",
       "      <td>тво</td>\n",
       "      <td>0.258267</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.236742</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.231846</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.247365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>моч</td>\n",
       "      <td>0.179664</td>\n",
       "      <td>наш</td>\n",
       "      <td>0.203139</td>\n",
       "      <td>сво</td>\n",
       "      <td>0.197972</td>\n",
       "      <td>наш</td>\n",
       "      <td>0.157783</td>\n",
       "      <td>ваш</td>\n",
       "      <td>0.200418</td>\n",
       "      <td>человек</td>\n",
       "      <td>0.234945</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.179488</td>\n",
       "      <td>тво</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.177318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>прост</td>\n",
       "      <td>0.164381</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.189513</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.190973</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.146096</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.175929</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.225443</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.167179</td>\n",
       "      <td>сво</td>\n",
       "      <td>0.182089</td>\n",
       "      <td>прост</td>\n",
       "      <td>0.161198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>котор</td>\n",
       "      <td>0.162160</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.172998</td>\n",
       "      <td>тво</td>\n",
       "      <td>0.180974</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.136002</td>\n",
       "      <td>сво</td>\n",
       "      <td>0.174522</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.169298</td>\n",
       "      <td>прост</td>\n",
       "      <td>0.164889</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>моч</td>\n",
       "      <td>0.153284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>знат</td>\n",
       "      <td>0.151142</td>\n",
       "      <td>знат</td>\n",
       "      <td>0.158134</td>\n",
       "      <td>знат</td>\n",
       "      <td>0.147979</td>\n",
       "      <td>ход</td>\n",
       "      <td>0.129095</td>\n",
       "      <td>котор</td>\n",
       "      <td>0.168892</td>\n",
       "      <td>сво</td>\n",
       "      <td>0.164980</td>\n",
       "      <td>оказыва</td>\n",
       "      <td>0.153152</td>\n",
       "      <td>говор</td>\n",
       "      <td>0.142919</td>\n",
       "      <td>каза</td>\n",
       "      <td>0.143613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Words-я  TF-IDF-я Words-мы  TF-IDF-мы Words-ты  TF-IDF-ты Words-нас  \\\n",
       "1  человек  0.232178  человек   0.215112  человек   0.236633   человек   \n",
       "2      моч  0.179664      наш   0.203139      сво   0.197972       наш   \n",
       "3    прост  0.164381    котор   0.189513      моч   0.190973       моч   \n",
       "4    котор  0.162160      моч   0.172998      тво   0.180974     котор   \n",
       "5     знат  0.151142     знат   0.158134     знат   0.147979       ход   \n",
       "\n",
       "   TF-IDF-нас Words-вы  TF-IDF-вы Words-тебя  TF-IDF-тебя Words-меня  \\\n",
       "1    0.201346  человек   0.269101        тво     0.258267    человек   \n",
       "2    0.157783      ваш   0.200418    человек     0.234945      котор   \n",
       "3    0.146096      моч   0.175929        моч     0.225443        моч   \n",
       "4    0.136002      сво   0.174522      котор     0.169298      прост   \n",
       "5    0.129095    котор   0.168892        сво     0.164980    оказыва   \n",
       "\n",
       "   TF-IDF-меня Words-тебе  TF-IDF-тебе Words-мне  TF-IDF-мне  \n",
       "1     0.236742    человек     0.231846   человек    0.247365  \n",
       "2     0.179488        тво     0.189500     котор    0.177318  \n",
       "3     0.167179        сво     0.182089     прост    0.161198  \n",
       "4     0.164889        моч     0.155623       моч    0.153284  \n",
       "5     0.153152      говор     0.142919      каза    0.143613  "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# https://towardsdatascience.com/higher-accuracy-and-less-process-time-in-text-classification-with-lda-and-tf-idf-d2d949e344c3\r\n",
    "# https://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\r\n",
    "# https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\r\n",
    "\r\n",
    "# The transform() function computes the tfidf frequency of each word in the bag of word.\r\n",
    "# Now our aim is to compare the document D2 with D1. It means we want to see how many words\r\n",
    "# of D1 match up with D2. Thats why we perform fit_transform() on D1 and then only the transform()\r\n",
    "# function on D2 would apply the bag of words of D1 and count the inverse frequency of tokens in D2.\r\n",
    "# This would give the relative comparison of D1 against D2.\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=8, stop_words = 'english' )\r\n",
    "\r\n",
    "tfidf_matrix = tfidf.fit_transform(token_corpus.values())\r\n",
    "feature_names = tfidf.get_feature_names()\r\n",
    "\r\n",
    "response_я = tfidf.transform([token_я])\r\n",
    "response_мы = tfidf.transform([token_мы])\r\n",
    "response_ты  = tfidf.transform([token_ты])\r\n",
    "response_нас = tfidf.transform([token_нас])\r\n",
    "response_вы = tfidf.transform([token_вы])\r\n",
    "response_тебя = tfidf.transform([token_тебя])\r\n",
    "response_меня = tfidf.transform([token_меня])\r\n",
    "response_тебе = tfidf.transform([token_тебе])\r\n",
    "response_мне = tfidf.transform([token_мне])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Size of corpus\r\n",
    "print(tfidf_matrix.shape)\r\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf.get_feature_names())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(122, 13192)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from nltk.stem import SnowballStemmer\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import plotly.graph_objects as go\r\n",
    "\r\n",
    "# Links used for understanding tfidf\r\n",
    "# https://towardsdatascience.com/higher-accuracy-and-less-process-time-in-text-classification-with-lda-and-tf-idf-d2d949e344c3\r\n",
    "# https://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php\r\n",
    "# https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\r\n",
    "\r\n",
    "corpus = files_to_corpus()\r\n",
    "\r\n",
    "# TfidfVect object with variables specified.\r\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, analyzer='word',\r\n",
    "                        sublinear_tf=True, smooth_idf=True, use_idf=True)\r\n",
    "\r\n",
    "# Forms a tfidf Matrix using the text from the files.\r\n",
    "tfidf_matrix = tfidf.fit_transform(corpus.values())\r\n",
    "feature_names = tfidf.get_feature_names()\r\n",
    "\r\n",
    "#  \r\n",
    "for i in range(len(corpus)):\r\n",
    "    feature_index = tfidf_matrix[i,:].nonzero()[1]\r\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\r\n",
    "\r\n",
    "# sum tfidf frequency of each term through documents\r\n",
    "sums = tfidf_matrix.sum(axis=0)\r\n",
    "\r\n",
    "# connecting term to its sum frequency\r\n",
    "data = []\r\n",
    "for col, term in enumerate(feature_names):\r\n",
    "    if col > 1000 and len(term) > 3:\r\n",
    "        data.append( (str(posTag(term)), sums[0,col] ))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'files_to_corpus' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19212/4158733378.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles_to_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# TfidfVect object with variables specified.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files_to_corpus' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Saves to CSV\r\n",
    "compression_opts = dict(method='zip',\r\n",
    "                        archive_name='out.csv')  \r\n",
    "\r\n",
    "ordering.to_csv('out.zip', index=False,\r\n",
    "          compression=compression_opts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read SCHEMA of json file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# Open json, print json schema\r\n",
    "df_idf = pd.read_json(\"../russianDataSets/ru/2020-03-01.json\",lines=True)\r\n",
    "\r\n",
    "# print schema\r\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\r\n",
    "print(\"Number of questions,columns=\",df_idf.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Schema:\n",
      "\n",
      " quote_count                                int64\n",
      "contributors                             float64\n",
      "truncated                                   bool\n",
      "text                                      object\n",
      "is_quote_status                             bool\n",
      "in_reply_to_status_id                    float64\n",
      "reply_count                                int64\n",
      "id                                         int64\n",
      "favorite_count                             int64\n",
      "created_at_src                            object\n",
      "collected_by                              object\n",
      "retweeted                                   bool\n",
      "investigationId                            int64\n",
      "coordinates                               object\n",
      "timestamp_ms                      datetime64[ns]\n",
      "entities                                  object\n",
      "in_reply_to_screen_name                   object\n",
      "id_str                                     int64\n",
      "text_translation                          object\n",
      "retweet_count                              int64\n",
      "stored_at                    datetime64[ns, UTC]\n",
      "in_reply_to_user_id                      float64\n",
      "favorited                                   bool\n",
      "retweeted_status                          object\n",
      "text_cleaned                              object\n",
      "user                                      object\n",
      "geo                                       object\n",
      "in_reply_to_user_id_str                  float64\n",
      "lang                                      object\n",
      "created_at                   datetime64[ns, UTC]\n",
      "filter_level                              object\n",
      "in_reply_to_status_id_str                float64\n",
      "place                                     object\n",
      "source                                    object\n",
      "_id                                       object\n",
      "display_text_range                        object\n",
      "extended_tweet                            object\n",
      "possibly_sensitive                       float64\n",
      "extended_entities                         object\n",
      "quoted_status_permalink                   object\n",
      "quoted_status_id                         float64\n",
      "quoted_status                             object\n",
      "quoted_status_id_str                     float64\n",
      "dtype: object\n",
      "Number of questions,columns= (1077, 43)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "def pre_process(text):\r\n",
    "    \r\n",
    "    # lowercase\r\n",
    "    text=text.lower()\r\n",
    "    if text[2] == \"rt\":\r\n",
    "        text=\"\"\r\n",
    "    #remove tags\r\n",
    "    text=re.sub(\"\",\"\",text)\r\n",
    "    \r\n",
    "    # remove special characters and digits\r\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\r\n",
    "    \r\n",
    "    return text\r\n",
    "\r\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import re\r\n",
    "\r\n",
    "def get_stop_words(stop_file_path):\r\n",
    "    \"\"\"load stop words \"\"\"\r\n",
    "    \r\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\r\n",
    "        stopwords = f.readlines()\r\n",
    "        stop_set = set(m.strip() for m in stopwords)\r\n",
    "        return frozenset(stop_set)\r\n",
    "\r\n",
    "#load a set of stop words\r\n",
    "stopwords=get_stop_words(\"../txts/2020-03-01.txt\")\r\n",
    "\r\n",
    "#get the text column \r\n",
    "docs=df_idf['text'].tolist()\r\n",
    "\r\n",
    "#create a vocabulary of words, \r\n",
    "#ignore words that appear in 85% of documents, \r\n",
    "#eliminate stop words\r\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\r\n",
    "word_count_vector=cv.fit_transform(docs)\r\n",
    "list(cv.vocabulary_.keys())[:10]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\c1635922\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning:\n",
      "\n",
      "Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['12', '13', '1400', '1595', '1600', '2008', '22', '2выздоровевшие', 'авто', 'астроном', 'блокчейн', 'вице', 'военно', 'военной', 'вторых', 'гей', 'демократа', 'док', 'доказательство', 'другому', 'душит', 'дь', 'европейска', 'женщину', 'журналистике', 'заболевшие', 'заек', 'захватчк', 'избавление', 'израиль', 'квартиры', 'коллега', 'коммунизм', 'крови', 'летняя', 'лужаек', 'марино', 'морских', 'морской', 'наркомана', 'недоразумение', 'нибудь', 'нон', 'ов', 'онлайн', 'оружии', 'отдохни', 'пекински', 'первых', 'пид', 'погибшие', 'подруга', 'порно', 'прежнему', 'премьер', 'прерогатива', 'пресс', 'причинно', 'пропаганда', 'рмот', 'салоном', 'сан', 'сканворды', 'следственных', 'службы', 'спаса', 'стопом', 'су', 'тату', 'тихо', 'толку', 'укро', 'чудо', 'чью', 'штаб', 'штирлицно', 'эксперта'] not in stop_words.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'shining_stri',\n",
       " 'coldabbess',\n",
       " 'тёмная',\n",
       " 'даёт',\n",
       " 'плю',\n",
       " 'https',\n",
       " 'co',\n",
       " 'oubahqve',\n",
       " 'vovablya']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "\r\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\r\n",
    "tfidf_transformer.fit(word_count_vector)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read test docs into a dataframe and concatenate title and body\r\n",
    "df_test=pd.read_json(\"../russianDataSets/ru/2020-03-02.json\",lines=True)\r\n",
    "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\r\n",
    "\r\n",
    "# get test docs into a list\r\n",
    "docs_test=df_test['text'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}