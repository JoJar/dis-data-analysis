{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "import lexical_specificity\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all words in the corpus and how many times they appear\n",
    "def dictionary_count(corpus):\n",
    "    word_counts = {}\n",
    "    for i in corpus:\n",
    "        ls = corpus.get(i)\n",
    "        saver = Counter(ls)\n",
    "        word_counts[i] = saver\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_word_freq(word, corpus_words_count=corpus_words_count):\n",
    "    count = 0\n",
    "    for filename in corpus_words_count:\n",
    "        cor = corpus_words_count.get(filename)\n",
    "        if cor.get(word) != None:\n",
    "            count += cor.get(word)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sub corpus as a string\n",
    "def get_sub_corpus(pronoun):\n",
    "    corpus_dict = {}\n",
    "    ftr_dir='../ftrs/Pronouns/'+ pronoun\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    words = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        for j in read_ftr[\"text\"]:\n",
    "            for k in j:\n",
    "                if k not in exclude:\n",
    "                    words.append(k)\n",
    "           \n",
    "    #corpus_dict[pronoun] = words\n",
    "    return words\n",
    "\n",
    "def get_sub_corpus_length(subcorpus):\n",
    "    return len(subcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_corpus()\n",
    "corpus_length = get_corpus_length(corpus)\n",
    "corpus_words_count = dictionary_count(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1643400\n"
     ]
    }
   ],
   "source": [
    "print(corpus_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['а', 'в', 'г', 'е', 'ж', 'и', 'к', 'м', 'о', 'с', 'т', 'у', 'я', 'бы', 'во', 'вы', 'да', 'до', 'ее', 'ей', 'ею', 'её', 'же', 'за', 'из', 'им', 'их', 'ли', 'мы', 'на', 'не', 'ни', 'но', 'ну', 'нх', 'об', 'он', 'от', 'по', 'со', 'та', 'те', 'то', 'ту', 'ты', 'уж', 'без', 'был', 'вам', 'вас', 'ваш', 'вон', 'вот', 'все', 'всю', 'вся', 'всё', 'где', 'год', 'два', 'две', 'дел', 'для', 'его', 'ему', 'еще', 'ещё', 'или', 'ими', 'имя', 'как', 'кем', 'ком', 'кто', 'лет', 'мне', 'мог', 'мож', 'мои', 'мой', 'мор', 'моя', 'моё', 'над', 'нам', 'нас', 'наш', 'нее', 'ней', 'нем', 'нет', 'нею', 'неё', 'них', 'оба', 'она', 'они', 'оно', 'под', 'пор', 'при', 'про', 'раз', 'сам', 'сих', 'так', 'там', 'тем', 'тех', 'том', 'тот', 'тою', 'три', 'тут', 'уже', 'чем', 'что', 'эта', 'эти', 'это', 'эту', 'алло', 'буду', 'будь', 'бывь', 'была', 'были', 'было', 'быть', 'вами', 'ваша', 'ваше', 'ваши', 'ведь', 'весь', 'вниз', 'всем', 'всех', 'всею', 'года', 'году', 'даже', 'двух', 'день', 'если', 'есть', 'зато', 'кого', 'кому', 'куда', 'лишь', 'люди', 'мало', 'меля', 'меня', 'мимо', 'мира', 'мной', 'мною', 'мочь', 'надо', 'нами', 'наша', 'наше', 'наши', 'него', 'нему', 'ниже', 'ними', 'один', 'пока', 'пора', 'пять', 'рано', 'сама', 'сами', 'само', 'саму', 'свое', 'свои', 'свою', 'себе', 'себя', 'семь', 'стал', 'суть', 'твой', 'твоя', 'твоё', 'тебе', 'тебя', 'теми', 'того', 'тоже', 'тому', 'туда', 'хоть', 'хотя', 'чаще', 'чего', 'чему', 'чтоб', 'чуть', 'этим', 'этих', 'этой', 'этом', 'этот', 'более', 'будем', 'будет', 'будто', 'будут', 'вверх', 'вдали', 'вдруг', 'везде', 'внизу', 'время', 'всего', 'всеми', 'всему', 'всюду', 'давно', 'даром', 'долго', 'друго', 'жизнь', 'занят', 'затем', 'зачем', 'здесь', 'иметь', 'какая', 'какой', 'когда', 'кроме', 'лучше', 'между', 'менее', 'много', 'могут', 'может', 'можно', 'можхо', 'назад', 'низко', 'нужно', 'одной', 'около', 'опять', 'очень', 'перед', 'позже', 'после', 'потом', 'почти', 'пятый', 'разве', 'рядом', 'самим', 'самих', 'самой', 'самом', 'своей', 'своих', 'сеаой', 'снова', 'собой', 'собою', 'такая', 'также', 'такие', 'такое', 'такой', 'тобой', 'тобою', 'тогда', 'тысяч', 'уметь', 'часто', 'через', 'чтобы', 'шесть', 'этими', 'этого', 'этому', 'близко', 'больше', 'будете', 'будешь', 'бывает', 'важная', 'важное', 'важные', 'важный', 'вокруг', 'восемь', 'всегда', 'второй', 'далеко', 'дальше', 'девять', 'десять', 'должно', 'другая', 'другие', 'других', 'другое', 'другой', 'занята', 'занято', 'заняты', 'значит', 'именно', 'иногда', 'каждая', 'каждое', 'каждые', 'каждый', 'кругом', 'меньше', 'начала', 'нельзя', 'нибудь', 'никуда', 'ничего', 'обычно', 'однако', 'одного', 'отсюда', 'первый', 'потому', 'почему', 'просто', 'против', 'раньше', 'самими', 'самого', 'самому', 'своего', 'сейчас', 'сказал', 'совсем', 'теперь', 'только', 'третий', 'хорошо', 'хотеть', 'хочешь', 'четыре', 'шестой', 'восьмой', 'впрочем', 'времени', 'говорил', 'говорит', 'девятый', 'десятый', 'кажется', 'конечно', 'которая', 'которой', 'которые', 'который', 'которых', 'наверху', 'наконец', 'недавно', 'немного', 'нередко', 'никогда', 'однажды', 'посреди', 'сегодня', 'седьмой', 'сказала', 'сказать', 'сколько', 'слишком', 'сначала', 'спасибо', 'человек', 'двадцать', 'довольно', 'которого', 'наиболее', 'недалеко', 'особенно', 'отовсюду', 'двадцатый', 'миллионов', 'несколько', 'прекрасно', 'процентов', 'четвертый', 'двенадцать', 'непрерывно', 'пожалуйста', 'пятнадцать', 'семнадцать', 'тринадцать', 'двенадцатый', 'одиннадцать', 'пятнадцатый', 'семнадцатый', 'тринадцатый', 'шестнадцать', 'восемнадцать', 'девятнадцать', 'одиннадцатый', 'четырнадцать', 'шестнадцатый', 'восемнадцатый', 'девятнадцатый', 'действительно', 'четырнадцатый', 'многочисленная', 'многочисленное', 'многочисленные', 'многочисленный', \"я\", \"мы\", \"ты\", \"oни\", \"нас\", \"вы\", \"тебя\", \"меня\", \"тебе\", \"вами\", \"мне\", \"тысяч\", \"котор\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to implement lexical_specificty.py\n",
    "# Function takes a subcorpus, the length of the sub corpus and a list of all words in the subcorpus with duplicates removed.\n",
    "# Lexical Specificity is calculated on the main corpus length, the subcorpus length, the count of the word in the main corpus and the count of the word in the sub corpus.\n",
    "# The words in the main corpus are pre counted to save time.\n",
    "def get_lexical_spec(pronoun, subcorpus_words, subcorpus, subcorpus_length, subcorpus_no_duplicates):\n",
    "    subcorpus_word_freq = count_word_sub_corpus(subcorpus)\n",
    "    # count all words in subcorpus\n",
    "    word_tpl = []\n",
    "    \n",
    "    for w in tqdm(subcorpus_no_duplicates):\n",
    "            sc_w = subcorpus_word_freq.get(w)\n",
    "            c_w = 0\n",
    "\n",
    "            for i in corpus_words_count:\n",
    "                c = corpus_words_count.get(i)\n",
    "                cw = c.get(w)\n",
    "                if cw != None:\n",
    "                    c_w+=cw\n",
    "            if sc_w != None:\n",
    "                    word_tpl.append((w, lexical_specificity.lexical_specificity(corpus_length, subcorpus_length, c_w, sc_w)))\n",
    "\n",
    "    return word_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_words(pronoun):\n",
    "    ftr_dir='../ftrs/Pronouns/' + pronoun\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    flat_list = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "        flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41352/41352 [00:00<00:00, 72360.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_я = get_sub_corpus('я')\n",
    "subcorpus_no_duplicates_я = list( dict.fromkeys(subcorpus_я) )\n",
    "subcorpus_length_я = get_sub_corpus_length(subcorpus_я)\n",
    "\n",
    "ls_я = get_lexical_spec('я', get_list_of_words('я'), subcorpus_я, subcorpus_length_я,subcorpus_no_duplicates_я)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17719/17719 [00:00<00:00, 57126.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_ты= get_sub_corpus('ты')\n",
    "subcorpus_no_duplicates_ты = list( dict.fromkeys(subcorpus_ты) )\n",
    "subcorpus_length_ты = get_sub_corpus_length(subcorpus_ты)\n",
    "\n",
    "ls_ты = get_lexical_spec('ты', get_list_of_words('ты'), subcorpus_ты, subcorpus_length_ты, subcorpus_no_duplicates_ты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8881/8881 [00:00<00:00, 80224.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_тебя= get_sub_corpus('тебя')\n",
    "subcorpus_no_duplicates_тебя = list( dict.fromkeys(subcorpus_тебя) )\n",
    "subcorpus_length_тебя= get_sub_corpus_length(subcorpus_тебя)\n",
    "\n",
    "ls_тебя = get_lexical_spec('тебя', get_list_of_words('тебя'), subcorpus_тебя, subcorpus_length_тебя, subcorpus_no_duplicates_тебя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77100/77100 [00:00<00:00, 95206.77it/s] \n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_нас= get_sub_corpus('нас')\n",
    "subcorpus_no_duplicates_нас = list( dict.fromkeys(subcorpus_нас) )\n",
    "subcorpus_length_нас= get_sub_corpus_length(subcorpus_нас)\n",
    "\n",
    "ls_нас = get_lexical_spec('нас', get_list_of_words('нас'), subcorpus_нас, subcorpus_length_нас, subcorpus_no_duplicates_нас)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15735/15735 [00:00<00:00, 96202.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_мы= get_sub_corpus('мы')\n",
    "subcorpus_no_duplicates_мы = list( dict.fromkeys(subcorpus_мы) )\n",
    "subcorpus_length_мы= get_sub_corpus_length(subcorpus_мы)\n",
    "\n",
    "ls_мы = get_lexical_spec('мы', get_list_of_words('мы'), subcorpus_мы, subcorpus_length_мы, subcorpus_no_duplicates_мы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14806/14806 [00:00<00:00, 73481.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_мне= get_sub_corpus('мне')\n",
    "subcorpus_no_duplicates_мне = list( dict.fromkeys(subcorpus_мне) )\n",
    "subcorpus_length_мне= get_sub_corpus_length(subcorpus_мне)\n",
    "\n",
    "ls_мне = get_lexical_spec('мне', get_list_of_words('мне'), subcorpus_мне, subcorpus_length_мне, subcorpus_no_duplicates_мне)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48962/48962 [00:00<00:00, 66610.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_меня= get_sub_corpus('меня')\n",
    "subcorpus_no_duplicates_меня = list( dict.fromkeys(subcorpus_меня) )\n",
    "subcorpus_length_меня= get_sub_corpus_length(subcorpus_меня)\n",
    "\n",
    "ls_меня = get_lexical_spec('меня', get_list_of_words('меня'), subcorpus_меня, subcorpus_length_меня, subcorpus_no_duplicates_меня)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96917/96917 [00:01<00:00, 91416.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_их= get_sub_corpus('их')\n",
    "subcorpus_no_duplicates_их = list( dict.fromkeys(subcorpus_их) )\n",
    "subcorpus_length_их= get_sub_corpus_length(subcorpus_их)\n",
    "\n",
    "ls_их = get_lexical_spec('их', get_list_of_words('их'), subcorpus_их, subcorpus_length_их, subcorpus_no_duplicates_их)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84470/84470 [00:01<00:00, 77774.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_вы= get_sub_corpus('вы')\n",
    "subcorpus_no_duplicates_вы = list( dict.fromkeys(subcorpus_вы) )\n",
    "subcorpus_length_вы= get_sub_corpus_length(subcorpus_вы)\n",
    "\n",
    "ls_вы = get_lexical_spec('вы', get_list_of_words('вы'), subcorpus_вы, subcorpus_length_вы, subcorpus_no_duplicates_вы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105611/105611 [00:00<00:00, 114847.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_вас= get_sub_corpus('вас')\n",
    "subcorpus_no_duplicates_вас = list( dict.fromkeys(subcorpus_вас) )\n",
    "subcorpus_length_вас= get_sub_corpus_length(subcorpus_вас)\n",
    "\n",
    "ls_вас= get_lexical_spec('вас', get_list_of_words('вас'), subcorpus_вас, subcorpus_length_вас, subcorpus_no_duplicates_вас)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22204/22204 [00:00<00:00, 83073.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "\n",
    "subcorpus_oни= get_sub_corpus('oни')\n",
    "subcorpus_no_duplicates_oни = list( dict.fromkeys(subcorpus_oни) )\n",
    "subcorpus_length_oни= get_sub_corpus_length(subcorpus_oни)\n",
    "\n",
    "ls_oни= get_lexical_spec('oни', get_list_of_words('oни'), subcorpus_oни, subcorpus_length_oни, subcorpus_no_duplicates_oни)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_100_words(pronoun):\n",
    "    # adjustable file in ftr format\n",
    "    ftr_dir = '../ftrs/Pronouns/' + pronoun\n",
    "\n",
    "    # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    # reads contents of save_dir.ftr\n",
    "    flat_list = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "        flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "\n",
    "        saver = Counter(\" \".join(flat_list).split()).most_common(200)\n",
    "        return saver[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\"oни\", \"вас\",  \"вы\", \"их\",\"меня\", \"мне\", \"мы\",\"нас\", \"тебя\",\"ты\", \"я\"]\n",
    "exclude_oни= most_common_100_words(\"oни\")\n",
    "exclude_вас= most_common_100_words(\"вас\")\n",
    "exclude_вы= most_common_100_words(\"вы\")\n",
    "exclude_их = most_common_100_words(\"их\")\n",
    "exclude_меня = most_common_100_words(\"меня\")\n",
    "exclude_мне = most_common_100_words(\"мне\")\n",
    "exclude_мы = most_common_100_words(\"мы\")\n",
    "exclude_нас = most_common_100_words(\"нас\")\n",
    "exclude_тебя = most_common_100_words(\"тебя\")\n",
    "exclude_ты = most_common_100_words(\"ты\")\n",
    "exclude_я = most_common_100_words(\"я\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_20_ls(list_of_tuples):\n",
    "    sorted_ls = []\n",
    "    filtered_data = []\n",
    "    for i in list_of_tuples:\n",
    "        if i[0] not in exclude and len(i[0]) >= 4:\n",
    "            filtered_data.append(i)\n",
    "\n",
    "    sorted_by_second = sorted(filtered_data, reverse=True, key=lambda tup: tup[1])\n",
    "\n",
    "    sorted_ls += sorted_by_second[:10]\n",
    "\n",
    "    return sorted_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_oни= top_20_ls(ls_oни)\n",
    "top_20_вас= top_20_ls(ls_вас)\n",
    "top_20_вы= top_20_ls(ls_вы)\n",
    "top_20_их = top_20_ls(ls_их)\n",
    "top_20_меня = top_20_ls(ls_меня)\n",
    "top_20_мне = top_20_ls(ls_мне)\n",
    "top_20_мы = top_20_ls(ls_мы)\n",
    "top_20_нас = top_20_ls(ls_нас)\n",
    "top_20_тебя = top_20_ls(ls_тебя)\n",
    "top_20_ты = top_20_ls(ls_ты)\n",
    "top_20_я = top_20_ls(ls_я)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "имеют 53.67671588004953\n",
      "знают 35.20422598187604\n",
      "дела 33.85189842575182\n",
      "погуб 32.7996698616399\n",
      "друг 30.321743619371926\n",
      "понима 25.775524779726418\n",
      "милосердн 24.5021765604342\n",
      "живут 22.678458135328917\n",
      "прост 22.346744500743984\n",
      "омоновц 22.13084191282199\n"
     ]
    }
   ],
   "source": [
    "def data_printer(data):\n",
    "    for i in data:\n",
    "        j,k = i\n",
    "        print(j,k)\n",
    "data_printer(top_20_oни)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_corpus():\n",
    "    corpus_dict = {}\n",
    "\n",
    "    ftr_dir='../ftrs/individual-ftrs/'\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "    \n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        words = []\n",
    "        for j in read_ftr[\"text\"]:\n",
    "            for k in j:\n",
    "                if k not in exclude:\n",
    "                    words.append(k)\n",
    "           \n",
    "            corpus_dict[i[:10]] = words\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus length = M\n",
    "def get_corpus_length(corpus):\n",
    "    total = 0\n",
    "    \n",
    "    for i in corpus:\n",
    "        total+=len(corpus[i])\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_sub_corpus(subcorpus):\n",
    "    word_counts = {}\n",
    "    saver = Counter(subcorpus)\n",
    "\n",
    "    return saver"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
