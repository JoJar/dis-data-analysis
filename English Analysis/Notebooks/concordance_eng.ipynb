{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from prepro.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nltk, glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from num2words import num2words\n",
    "import import_ipynb\n",
    "import prepro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing before concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1999000/2000000 [6:51:49<00:12, 80.90it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = '../textFiles/EN-03-2020-No-Retweets.txt'\n",
    "tokens = \"\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(tqdm(f.readlines(), total=2000000)):\n",
    "        tokens+=line\n",
    "        if i == 1999000:\n",
    "            tokens = prepro.preprocess(tokens)\n",
    "            tokens = nltk.word_tokenize(tokens)\n",
    "            break\n",
    "\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\\2020-03-25-pre-I-pronoun.ftr\n",
      "I\\2020-03-26-pre-I-pronoun.ftr\n",
      "I\\2020-03-27-pre-I-pronoun.ftr\n",
      "I\\2020-03-28-pre-I-pronoun.ftr\n",
      "I\\2020-03-29-pre-I-pronoun.ftr\n",
      "I\\2020-03-30-pre-I-pronoun.ftr\n",
      "I\\2020-03-31-pre-I-pronoun.ftr\n",
      "I\\2020-04-01-pre-I-pronoun.ftr\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "map() must have at least two arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19312/3877054468.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mget_all_phases_containing_tar_wrd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19312/3877054468.py\u001b[0m in \u001b[0;36mget_all_phases_containing_tar_wrd\u001b[1;34m(target_word, tar_passage, left_margin, right_margin)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConcordanceIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"HI\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: map() must have at least two arguments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "ftr_dir = '../all-en-ftrs/Words/I'\n",
    "s = \"\"\n",
    "\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "for file in file_list:\n",
    "    print(file[21:])\n",
    "\n",
    "    read_ftr = pd.read_feather(file, columns=None, use_threads=True)\n",
    "    save_dir = '../March-No-Retweets/' + file[21:].replace('.ftr', '.txt')\n",
    "    s = read_ftr[\"text\"]\n",
    "    \n",
    "\n",
    "    def get_all_phases_containing_tar_wrd(target_word, tar_passage, left_margin = 10, right_margin = 10):\n",
    "        for index, tokens in tar_passage.items():\n",
    "            text = nltk.Text(tokens)\n",
    "            c = nltk.ConcordanceIndex(target_word)\n",
    "            print(\"HI\", text.tokens[offset for offset in c.offsets(target_word)])\n",
    "                                \n",
    "        return string\n",
    "\n",
    "get_all_phases_containing_tar_wrd(\"i\", s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordanceToFile(conc):\n",
    "    filename = '../textFiles/Pronoun/Large/concordance_' + conc + '.txt'\n",
    "    conc_save = text.concordance_list(conc, lines=1999000)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf8') as fin:\n",
    "        for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\n",
    "                fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function only for I as there are special exceptions including ive,im,ill\n",
    "\n",
    "def concordance_to_file_i():\n",
    "    conc_list = [\"i\", \"im\", \"ill\", \"ive\"]\n",
    "    filename = '../textFiles/Pronoun/Large/concordance_' + 'i' + '.txt'\n",
    "    \n",
    "    for conc in conc_list:\n",
    "        conc_save = text.concordance_list(conc, lines=1999000)\n",
    "    \n",
    "        with open(filename, 'a', encoding='utf8') as fin:\n",
    "            for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\n",
    "                    fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")\n",
    "\n",
    "\n",
    "def concordance_to_file_you():\n",
    "    conc_list = [\"you\", \"youve\", \"youll\", \"youre\"]\n",
    "    filename = '../textFiles/Pronoun/Large/concordance_' + 'you' + '.txt'\n",
    "    \n",
    "    for conc in conc_list:\n",
    "        conc_save = text.concordance_list(conc, lines=1999000)\n",
    "\n",
    "        with open(filename, 'a', encoding='utf8') as fin:\n",
    "            for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\n",
    "                    fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")\n",
    "\n",
    "def concordance_to_file_we():\n",
    "    conc_list = [\"we\", \"weve\", \"well\", \"were\"]\n",
    "    filename = '../textFiles/Pronoun/Large/concordance_' + 'we' + '.txt'\n",
    "    \n",
    "    for conc in conc_list:\n",
    "        conc_save = text.concordance_list(conc, lines=1999000)\n",
    "    \n",
    "        with open(filename, 'a', encoding='utf8') as fin:\n",
    "            for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\n",
    "                    fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")\n",
    "\n",
    "def concordance_to_file_they():\n",
    "    conc_list = [\"they\", \"theyve\", \"theyll\", \"theyre\"]\n",
    "    filename = '../textFiles/Pronoun/Large/concordance_' + 'they' + '.txt'\n",
    "    \n",
    "    for conc in conc_list:\n",
    "        conc_save = text.concordance_list(conc, lines=1999000)\n",
    "\n",
    "        with open(filename, 'a', encoding='utf8') as fin:\n",
    "            for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\n",
    "                    fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordance\n",
    "Functions which run the concordance on a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_concordance(list_of_words):\n",
    "    for i in list_of_words:\n",
    "        concordanceToFile(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13143/13143 [00:00<00:00, 44530.83it/s]\n",
      "100%|██████████| 3045/3045 [00:00<00:00, 27415.75it/s]\n",
      "100%|██████████| 359/359 [00:00<00:00, 18659.37it/s]\n",
      "100%|██████████| 598/598 [00:00<00:00, 23308.83it/s]\n",
      "100%|██████████| 3205/3205 [00:00<00:00, 27342.55it/s]\n",
      "100%|██████████| 19570/19570 [00:00<00:00, 62259.48it/s]\n",
      "100%|██████████| 268/268 [00:00<00:00, 45688.47it/s]\n",
      "100%|██████████| 182/182 [00:00<00:00, 31007.08it/s]\n",
      "100%|██████████| 1833/1833 [00:00<00:00, 39557.10it/s]\n",
      "100%|██████████| 6151/6151 [00:00<00:00, 50978.43it/s]\n",
      "100%|██████████| 149/149 [00:00<00:00, 49884.36it/s]\n",
      "100%|██████████| 1202/1202 [00:00<00:00, 47368.77it/s]\n",
      "100%|██████████| 1965/1965 [00:00<00:00, 26890.69it/s]\n",
      "100%|██████████| 8638/8638 [00:00<00:00, 46792.19it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 52353.44it/s]\n",
      "100%|██████████| 105/105 [00:00<00:00, 49701.15it/s]\n",
      "100%|██████████| 795/795 [00:00<00:00, 38890.05it/s]\n",
      "100%|██████████| 3192/3192 [00:00<00:00, 36272.60it/s]\n",
      "100%|██████████| 3148/3148 [00:00<00:00, 38166.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def pronoun_concordance():\n",
    "    pronouns = [\"i\", \"me\", \"you\", \"we\", \"they\", \"my\", \"us\"]\n",
    "\n",
    "    for i in pronouns:\n",
    "        if i == \"i\":\n",
    "            concordance_to_file_i()\n",
    "        elif i == \"you\":\n",
    "            concordance_to_file_you()\n",
    "        elif i == \"we\":\n",
    "            concordance_to_file_we()\n",
    "        elif i == \"they\":\n",
    "            concordance_to_file_they()\n",
    "        else:\n",
    "            concordanceToFile(i)\n",
    "\n",
    "pronoun_concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 38582.52it/s]\n",
      "100%|██████████| 1735/1735 [00:00<00:00, 54355.93it/s]\n",
      "100%|██████████| 2153/2153 [00:00<00:00, 53970.13it/s]\n",
      "100%|██████████| 2862/2862 [00:00<00:00, 64241.13it/s]\n",
      "100%|██████████| 796/796 [00:00<00:00, 57019.55it/s]\n",
      "100%|██████████| 1871/1871 [00:00<00:00, 64687.86it/s]\n",
      "100%|██████████| 1599/1599 [00:00<00:00, 66718.65it/s]\n",
      "100%|██████████| 1114/1114 [00:00<00:00, 50670.79it/s]\n",
      "100%|██████████| 3268/3268 [00:00<00:00, 61825.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def verb_concordance():\n",
    "    vocab = [\"believe\", \"think\", \"know\", \"tell\", \"need\", \"want\", \"read\", \"spreading\", \"stop\", \"give\"]\n",
    "\n",
    "    for i in verbs:\n",
    "        concordanceToFile(i)\n",
    "\n",
    "verb_concordance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 13117/35000 [00:00<00:00, 130222.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../March-No-Retweets/Pronoun\\\\I', '../March-No-Retweets/Pronoun\\\\Me', '../March-No-Retweets/Pronoun\\\\My', '../March-No-Retweets/Pronoun\\\\They', '../March-No-Retweets/Pronoun\\\\Us', '../March-No-Retweets/Pronoun\\\\We', '../March-No-Retweets/Pronoun\\\\You']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 17135/35000 [00:00<00:00, 111565.53it/s]\n",
      " 56%|█████▋    | 19718/35000 [00:00<00:00, 31383.05it/s]\n",
      " 58%|█████▊    | 20252/35000 [00:00<00:00, 22071.28it/s]\n",
      " 61%|██████▏   | 21492/35000 [00:00<00:00, 21621.88it/s]\n",
      " 65%|██████▍   | 22596/35000 [00:01<00:01, 11402.01it/s]\n",
      " 54%|█████▍    | 18911/35000 [00:01<00:01, 13048.13it/s]\n",
      " 59%|█████▉    | 20706/35000 [00:01<00:01, 11345.03it/s]\n",
      " 54%|█████▍    | 18852/35000 [00:01<00:01, 10243.86it/s]\n",
      " 12%|█▏        | 4335/35000 [00:00<00:00, 206972.28it/s]\n",
      " 13%|█▎        | 4608/35000 [00:00<00:00, 243982.94it/s]\n",
      " 14%|█▍        | 4933/35000 [00:00<00:00, 260343.03it/s]\n",
      " 16%|█▌        | 5564/35000 [00:00<00:00, 82043.49it/s]\n",
      " 16%|█▌        | 5582/35000 [00:00<00:00, 63603.23it/s]\n",
      " 13%|█▎        | 4417/35000 [00:00<00:00, 48667.58it/s]\n",
      " 15%|█▍        | 5201/35000 [00:00<00:00, 41062.73it/s]\n",
      " 14%|█▎        | 4742/35000 [00:00<00:00, 37932.24it/s]\n",
      " 12%|█▏        | 4062/35000 [00:00<00:00, 193957.91it/s]\n",
      " 13%|█▎        | 4482/35000 [00:00<00:00, 140448.34it/s]\n",
      " 12%|█▏        | 4299/35000 [00:00<00:00, 119737.78it/s]\n",
      " 13%|█▎        | 4519/35000 [00:00<00:00, 66604.09it/s]\n",
      " 14%|█▎        | 4810/35000 [00:00<00:00, 55414.99it/s]\n",
      " 12%|█▏        | 4105/35000 [00:00<00:00, 45232.39it/s]\n",
      " 14%|█▎        | 4742/35000 [00:00<00:00, 33243.34it/s]\n",
      " 12%|█▏        | 4118/35000 [00:00<00:01, 26634.77it/s]\n",
      " 30%|██▉       | 10444/35000 [00:00<00:00, 227711.47it/s]\n",
      " 38%|███▊      | 13183/35000 [00:00<00:00, 51434.05it/s]\n",
      " 38%|███▊      | 13167/35000 [00:00<00:00, 40617.11it/s]\n",
      " 42%|████▏     | 14769/35000 [00:00<00:00, 25932.05it/s]\n",
      " 43%|████▎     | 15192/35000 [00:00<00:01, 16683.60it/s]\n",
      " 40%|████      | 14030/35000 [00:00<00:01, 14810.05it/s]\n",
      " 41%|████      | 14380/35000 [00:01<00:01, 11818.39it/s]\n",
      " 37%|███▋      | 13027/35000 [00:01<00:02, 9437.91it/s]\n",
      " 11%|█         | 3893/35000 [00:00<00:00, 134604.15it/s]\n",
      " 14%|█▎        | 4770/35000 [00:00<00:00, 101824.74it/s]\n",
      " 15%|█▍        | 5110/35000 [00:00<00:00, 46563.39it/s]\n",
      " 14%|█▍        | 5023/35000 [00:00<00:00, 29978.20it/s]\n",
      " 15%|█▌        | 5332/35000 [00:00<00:01, 19583.25it/s]\n",
      " 13%|█▎        | 4612/35000 [00:00<00:01, 15730.75it/s]\n",
      " 13%|█▎        | 4607/35000 [00:00<00:00, 35523.25it/s]\n",
      " 13%|█▎        | 4709/35000 [00:00<00:01, 28273.00it/s]\n",
      " 22%|██▏       | 7863/35000 [00:00<00:00, 82988.75it/s]\n",
      " 28%|██▊       | 9730/35000 [00:00<00:00, 40648.80it/s]\n",
      " 28%|██▊       | 9875/35000 [00:00<00:00, 33793.10it/s]\n",
      " 31%|███       | 10732/35000 [00:00<00:01, 13638.52it/s]\n",
      " 31%|███       | 10805/35000 [00:00<00:01, 13997.29it/s]\n",
      " 26%|██▌       | 9018/35000 [00:00<00:02, 9892.88it/s]\n",
      " 27%|██▋       | 9605/35000 [00:00<00:01, 15924.29it/s]\n",
      " 26%|██▌       | 9007/35000 [00:00<00:01, 17417.54it/s]\n",
      " 58%|█████▊    | 20356/35000 [00:00<00:00, 128334.08it/s]\n",
      " 67%|██████▋   | 23605/35000 [00:00<00:00, 41900.47it/s]\n",
      " 71%|███████   | 24731/35000 [00:01<00:00, 20788.59it/s]\n",
      " 85%|████████▌ | 29783/35000 [00:01<00:00, 17570.12it/s]\n",
      " 79%|███████▉  | 27636/35000 [00:02<00:00, 11928.92it/s]\n",
      " 69%|██████▊   | 24043/35000 [00:02<00:01, 9807.46it/s] \n",
      " 73%|███████▎  | 25454/35000 [00:02<00:00, 10242.58it/s]\n",
      " 64%|██████▎   | 22311/35000 [00:02<00:01, 8721.61it/s]\n"
     ]
    }
   ],
   "source": [
    "conc_dict = {}\n",
    "pronouns = [\"i\", \"me\", \"my\", \"they\", \"us\", \"we\", \"you\"]\n",
    "\n",
    "file_path = '../March-No-Retweets/Pronoun'\n",
    "lt = [x[0] for x in os.walk(file_path)]\n",
    "print(lt[1:])\n",
    "for n, txt_dir in enumerate(lt[1:]):\n",
    "    # Gets pronoun from file name\n",
    "    pronoun = txt_dir[21:]\n",
    "\n",
    "    # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "    ftr_pattern = os.path.join(txt_dir, '*.txt')\n",
    "\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "    tokens = \"\"\n",
    "    for txt_file in file_list:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(tqdm(f.readlines(), total=35000)):\n",
    "                #remove duplicates\n",
    "                no_duplicates = list( dict.fromkeys(line.split()) )\n",
    "                tokens += \" \" + \" \".join(no_duplicates)\n",
    "                #tokens+=line\n",
    "\n",
    "    tokens = nltk.word_tokenize(tokens)\n",
    "    text = nltk.Text(tokens)\n",
    "    conc_dict[txt_file] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "\n",
      "\n",
      "Displaying 25 of 133391 matches:\n",
      "wrong i tell trump hater you democrat u we nee\n",
      "nformed instead fed lie twisting truth i wonder youd feel russia suppressed inf\n",
      " big impact spread doesnt comport fact i know you want me trump supporter every\n",
      "me trump supporter everything hurt you i say making wrong call trying rush reop\n",
      "py u give becoming right wing theorist i two thousand twenty doesnt even come c\n",
      "r upside luckily sick family end reign i see you future didnt voodoo even trust\n",
      " eleven several source couple week ago i mean best way spread eliminate people \n",
      "inate people supposed right disgusting i hope rat go going round parent run off\n",
      "e rat go going round parent run office i love fash you wont air beloved leader \n",
      " bob king reality trump two twenty law i make jersey read state joe remember sp\n",
      " joe remember spoke hour ago much week i call i never believe thousand six hund\n",
      "member spoke hour ago much week i call i never believe thousand six hundred wut\n",
      "done many bad thing me guess you think i let know mean girl pointing theory har\n",
      "ory harmful major hole didnt even well i encourage continue thats disclaimer ha\n",
      "e continue thats disclaimer harassment i individual extremely dangerous arm the\n",
      "ndestine member intelligence community i feel like missing piece keep seeing we\n",
      "ten curve people reading miss data see i agree pathetic we people prayer panic \n",
      "mp sanctuary city mayor governor china i trust part bot i declare nonsense base\n",
      " mayor governor china i trust part bot i declare nonsense based percentage test\n",
      " going window prince getting great day i mean you mind manila pig farmer theory\n",
      "mean you mind manila pig farmer theory i try yell my cousin niece they look me \n",
      "ist obviously comprehension skill lack i didnt say article thing obvious connot\n",
      "ct get two three talk topic check look i get you want stay relevant maybe consi\n",
      "thing going really bad one hear ask me i love watch trump roast medium face bri\n",
      "fession write you via opening far best i youd like say you back professionally \n",
      "me\n",
      "\n",
      "\n",
      "Displaying 25 of 39057 matches:\n",
      "d doesnt comport fact i know you want me one polled work de eu me gratis el ya\n",
      "now you want me one polled work de eu me gratis el ya me struck me my grandfat\n",
      "one polled work de eu me gratis el ya me struck me my grandfather physical bat\n",
      " work de eu me gratis el ya me struck me my grandfather physical battle warfar\n",
      " i try yell my cousin niece they look me like theorist million two hundred tho\n",
      " hundred thousand three dernier de si me bien cest un sur journal par tu ask m\n",
      "e bien cest un sur journal par tu ask me i love watch trump roast medium face \n",
      "eople profession write you via poi da me de eu da na me hundred yes government\n",
      "n write you via poi da me de eu da na me hundred yes government medium corpora\n",
      "ture capitalism ruthless take benefit me mentality driving people apart compet\n",
      "irit compassion relative keep sending me post theory corona virus i think they\n",
      "based spread hater trump patriot like me love country dont we headed real chan\n",
      "hundred two thousand five twenty plot me thing they want stop my rally everyth\n",
      "ock bring turn clown i continued make me go grade far biggest regret my ninete\n",
      "teen year retirer parent ce mien pour me de plus la force send me scientific p\n",
      "ce mien pour me de plus la force send me scientific proof report they you dont\n",
      "cience genetic difference real theory me struck my grandfather physical battle\n",
      "ental need banging summer em um ma ai me no keep spreading middle pandemic we \n",
      " seven hundred ten trillion debt tell me going well you pose threat narrative \n",
      " pose threat narrative apology enough me we conservative get shut they threate\n",
      "w would happen thousand seven hundred me today they shut death canada thirty t\n",
      "ositive i feeling left nobody talking me forced my wat hospital could post tru\n",
      "uld post trump little fire everywhere me every five minute disregard i first a\n",
      " full list collegiate athlete forgive me story intercept tara sexual assault t\n",
      " assault tool anything something tell me they trying take favorite losing trum\n",
      "my\n",
      "\n",
      "\n",
      "Displaying 25 of 35025 matches:\n",
      "struck me my grandfather physical battle warfare e\n",
      "no feeling similar fox technique spew my respect going window i try yell my co\n",
      "ew my respect going window i try yell my cousin niece they look me like theori\n",
      "me like theorist thats exactly i told my wife didnt believe said people would \n",
      "nt return dont agree decision however my point grandmother telling u blatant y\n",
      "y corona virus i think they mean well my anxiety pointing you said dont like n\n",
      " mad proved near claim also i consume my opinion trump action trump tape rant \n",
      "e twenty plot me thing they want stop my rally everything foe fail send contag\n",
      "d make me go grade far biggest regret my nineteen year shortage opportunity on\n",
      "fact objective actually u china would my point whats me struck my grandfather \n",
      " china would my point whats me struck my grandfather physical battle warfare e\n",
      "mpetition china may laced stuff sorry my thing normally actually getting home \n",
      "etting home thirteenth day quarantine my asleep two hundred cook start like sc\n",
      "ead everything wall keeper revolution my roommate asleep two hundred cook star\n",
      "wall keeper revolution you sound like my therapist big paper i say my job pay \n",
      "und like my therapist big paper i say my job pay anyone test positive you able\n",
      "back seriously show still sting i see my people refer fascist sympathizer frie\n",
      "er fascist sympathizer friend working my skill great yet palette i love you th\n",
      "feeling left nobody talking me forced my wat hospital could post trump thousan\n",
      "ry much humanly possible part waiting my stash face mask you know address nope\n",
      "get people got contact major hospital my daughter father law doctor i defer th\n",
      "e twenty plot me thing they want stop my rally everything foe fail send contag\n",
      "contagious jail lock bring turn thats my beef right i wish moment say reporter\n",
      "r zero ability honest lead disgust me my core sir epic failure leader human nu\n",
      "der human numerous situation actually my shouldnt alive right i know truly lif\n",
      "they\n",
      "\n",
      "\n",
      "Displaying 25 of 96768 matches:\n",
      "one depressing they leftist network get zero i wonder yo\n",
      "n foreign bill even trust government they always making mistake joke except fo\n",
      "e medium give unfiltered air context they show clip afterwards fact check time\n",
      "act check time we dont need reporter they leave want thousand fifteen polar be\n",
      "d sheep stop really autism made word they bob king reality trump two twenty la\n",
      "i individual extremely dangerous arm they sleeper cell clandestine member inte\n",
      "estine member intelligence community they going mail check address exist total\n",
      "arfare eighty year later we storming they could beat u mano professional journ\n",
      "eighty stupid forty thinking nothing they listen medium enemy people stop thin\n",
      "ium enemy people stop thinking press they nothing mouthpiece party point incre\n",
      "gure sudden change policy apparently they cant release anonymous without famil\n",
      "y consent i try yell my cousin niece they look me like theorist one thing make\n",
      " deep inside government doesnt trust they turn everything second wave virus hi\n",
      "ecently troll work derogatory insult they truth official cover china campaign \n",
      "y response virus model nation though they outbreak silenced wont pay tribute w\n",
      "vil medium keep saying china got top they tried hide nuclear virus different r\n",
      "ear job send massive straight people they want you gone suck president role mo\n",
      "people would notice distortion guess they seen time i understand they still th\n",
      "on guess they seen time i understand they still thousand student return dont a\n",
      " me post theory corona virus i think they mean well my anxiety republican carr\n",
      "e innocent voter two thousand twenty they think stupid corrupt something unite\n",
      "rupt something united eye everything they dont agree salon course they feed pe\n",
      "rything they dont agree salon course they feed peon audience theyll screw goon\n",
      "nti hong censor pro voice user lived they would use track people arrest spoke \n",
      "izen maybe we circulation fail thing they understand i almost never talk creat\n",
      "us\n",
      "\n",
      "\n",
      "Displaying 25 of 130 matches:\n",
      "o pushing simply finding u journalist us patient gave rise theory we time incl\n",
      "ntilator want nation go easter u made us medical response worse brit u flag hu\n",
      "grim utterly anyone abroad surely see us obi wan hope meaningful change progre\n",
      "ou actually believe got ta understand us western medium spout surging u overta\n",
      "l advice arent fox devotee understand us briefing try enhance chance cable ple\n",
      "mber communist time hundred seventeen us incompetence thats havoc chaos year t\n",
      "dont pay attention china spin machine us u liberal medium whitewash video left\n",
      "e think they would use chip control u us medium run believe they whats done pr\n",
      "ed me clumsy obvious staged explosion us may u many believe least open believi\n",
      " lying virus sat week you blame trump us u medium push snake oil salesman tell\n",
      "sponsible trump thousand nine hundred us system closer socialism difference go\n",
      "ockingbird medium make sound targeted us term we knew another lie divide u mos\n",
      "atory policy black reflection problem us platform ai ability case though real \n",
      "l clear theory fact government work u us television promote false reality they\n",
      "u without sorry you officially broken us economy gone back year next wait see \n",
      "e three different china help danger u us thinly veiled machine smear helper de\n",
      "inger china russia source virus viral us biggest foe along side sending medica\n",
      "er china week first case came country us slow china push u tie worsen politics\n",
      "didnt anything made russia still dont us child labor unfair china push u tie w\n",
      "fing already showing part i sure hope us emergency system communicate u would \n",
      "karo sarkar jo ability critically ass us authoritarian regime atrocity human r\n",
      "ech pharmacist feel safe main i worry us poor fine thank you president trump e\n",
      "sand two hundred forcing u quarantine us big data thing like large gathering e\n",
      "may die without ever tested i believe us true case least double whats hard fig\n",
      " skeptical china number id also argue us probably especially considering short\n",
      "we\n",
      "\n",
      "\n",
      "Displaying 25 of 68789 matches:\n",
      "ong i tell trump hater you democrat u we need know dont stop watching like way\n",
      "formed instead fed lie twisting truth we know friend church satan twitter ur w\n",
      "odoo yeah cause you lying cant peddle we hear president we dont need reporter \n",
      "u lying cant peddle we hear president we dont need reporter they leave want al\n",
      "stment china depend scheme succeeding we never forgive forget travesty win i f\n",
      "i feel like missing piece keep seeing we dont need since starting flatten curv\n",
      "eading miss data see i agree pathetic we people prayer panic medium never say \n",
      "stment china depend scheme succeeding we never forgive forget travesty win str\n",
      "ical battle warfare eighty year later we storming they could beat u mano wish \n",
      " could beat u mano wish network would we dont need wont pay tribute worker the\n",
      "tting laid make sure doesnt get arent we evil they want you gone suck presiden\n",
      " human thats lying po nothing fantasy we know specialty weve finally point rap\n",
      " luckily i found medical professional we trust control narrative cost well rea\n",
      "ative cost well really tax payer cool we get pay thousand one hundred eleven v\n",
      "y thousand one hundred eleven van say we believe twitter denounce real statist\n",
      "nce real statistic conservative idiot we need we learn pandemic exercise healt\n",
      " statistic conservative idiot we need we learn pandemic exercise health defens\n",
      "now there lot thats investment outlet we bottom way many unknown always better\n",
      "known always better little late early we one thousand die trump still calling \n",
      "larm actively pushing theory dementia we get trump deserve people get word rig\n",
      "rk time try lie confuse citizen maybe we circulation fail thing they understan\n",
      "stment china depend scheme succeeding we never forgive forget travesty win al \n",
      "ump patriot like me love country dont we headed real change research good cant\n",
      "one hundred true arm democratic party we knew week trying sink economy they fo\n",
      "ical battle warfare eighty year later we storming mental need banging summer a\n",
      "you\n",
      "\n",
      "\n",
      "Displaying 25 of 183709 matches:\n",
      "wrong i tell trump hater you democrat u we need know dont stop wat\n",
      "el russia suppressed information test you anti china arent election block preve\n",
      "act spread doesnt comport fact i know you want me trump supporter everything hu\n",
      "nt me trump supporter everything hurt you i say making wrong call trying rush r\n",
      "trying rush reopen country yeah trump you put regarding virus daily basis new s\n",
      "ly going stop dangerous statement lie you reply person sent say we know friend \n",
      "e luckily sick family end reign i see you future didnt voodoo elevated nominati\n",
      "odoo elevated nomination coming along you insult many intolerant ignorant thing\n",
      " trump take couple day talking medium you clown cry claim bizarre you wan na st\n",
      "ng medium you clown cry claim bizarre you wan na stay task support point contin\n",
      "nue make wildly assumption yeah cause you lying cant peddle we hear president y\n",
      "u lying cant peddle we hear president you innocent u people dont support great \n",
      "resident exactly weapon seeking smart you republican smart people laugh wow pla\n",
      "reading lie stupid corona i love fash you wont air beloved leader lie dont get \n",
      "eryone tune government briefing thank you meeting you cant even structure coher\n",
      "government briefing thank you meeting you cant even structure coherent sentence\n",
      "g come big mac mouth space month many you lost right peaceably assemble engage \n",
      "dred wut done many bad thing me guess you think i let know none thing moron wou\n",
      "port people continually produce least you accomplished reality thank you great \n",
      " least you accomplished reality thank you great question know hit nerve call go\n",
      " question know hit nerve call go show you account need suspended hey dont you s\n",
      "w you account need suspended hey dont you start working resume going need new j\n",
      "b get jail shut go back sucking thumb you think like god damn stop falling watc\n",
      "ng watch something medium really make you look stupid watch big meat dairy you \n",
      " you look stupid watch big meat dairy you lie listen there lot pretty sure you \n"
     ]
    }
   ],
   "source": [
    "def concord_dictionary():\n",
    "    pronouns = [\"i\", \"me\", \"my\", \"they\", \"us\", \"we\", \"you\"]\n",
    "    for n, i in enumerate(conc_dict):\n",
    "        print(pronouns[n])\n",
    "        print(\"\\n\")\n",
    "        conc_save = conc_dict.get(i).concordance(pronouns[n], lines=25)\n",
    "concord_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordanceToFile_verb(p_dict):\n",
    "    verbs = [\"believe\", \"think\", \"know\", \"tell\", \"need\", \"want\", \"read\", \"spreading\", \"stop\", \"give\"]\n",
    "\n",
    "    for n, i in enumerate(p_dict):\n",
    "        text = p_dict.get(i)\n",
    "        filename = '../March-No-Retweets/Concordance/concordance_' + verbs[n] + '.txt'\n",
    "        conc_save = text.concordance_list(verbs[n], lines=1999000)\n",
    "        for conc in conc_save:\n",
    "            c_line = conc.line\n",
    "        \n",
    "            with open(filename, 'a', encoding='utf8') as fin:\n",
    "                fin.write(c_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordanceToFile_think(p_dict):\n",
    "    verbs = [\"believe\", \"think\", \"know\"]\n",
    "\n",
    "    for v in verbs:\n",
    "        for n, i in enumerate(p_dict):\n",
    "            text = p_dict.get(i)\n",
    "            filename = '../March-No-Retweets/Concordance/Thoughts/concordance_' + v + '.txt'\n",
    "            conc_save = text.concordance_list(v, lines=1999000)\n",
    "            for conc in conc_save:\n",
    "                c_line = conc.line\n",
    "            \n",
    "                with open(filename, 'a', encoding='utf8') as fin:\n",
    "                    fin.write(c_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordanceToFile_think(conc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordanceToFile(p_dict):\n",
    "    pronouns = [\"i\", \"me\", \"my\", \"they\", \"us\", \"we\", \"you\"]\n",
    "\n",
    "    for n, i in enumerate(p_dict):\n",
    "        text = p_dict.get(i)\n",
    "        filename = '../March-No-Retweets/Concordance/concordance_' + pronouns[n] + '.txt'\n",
    "        conc_save = text.concordance_list(pronouns[n], lines=1999000)\n",
    "        for conc in conc_save:\n",
    "            c_line = conc.line\n",
    "        \n",
    "            with open(filename, 'a', encoding='utf8') as fin:\n",
    "                fin.write(c_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordanceToFile(conc_dict)\n",
    "\n",
    "# for i in conc_dict:\n",
    "#     print(conc_dict.get(i))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
