{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import json, os, glob\r\n",
    "import pandas as pd\r\n",
    "import feather\r\n",
    "from collections import Counter\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This nb contains the processes which save individual jsons to individual ftrs.\r\n",
    "# This nb also contains the processes which find whether a tweets contains...\r\n",
    "# ...or does not contain pronouns and saves this to their respective seperate file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "json_dir = '../jsons'\r\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\r\n",
    "json_pattern = os.path.join(json_dir, '*.json')\r\n",
    "\r\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\r\n",
    "file_list = glob.glob(json_pattern)\r\n",
    "\r\n",
    "# Opens the JSON file\r\n",
    "for file in file_list:\r\n",
    "    print(file)\r\n",
    "    rows = []\r\n",
    "    links = []\r\n",
    "    count=0\r\n",
    "\r\n",
    "    with open(file, 'r') as f:\r\n",
    "        # directory to save data to\r\n",
    "        save_dir = file.replace('.json', '.ftr')\r\n",
    "\r\n",
    "        for line in f:\r\n",
    "            if count > 10000:\r\n",
    "                break\r\n",
    "            count = 0\r\n",
    "            data = json.loads(line)\r\n",
    "\r\n",
    "            if count == 0:\r\n",
    "                # Retweet?\r\n",
    "                if data[\"text\"][:2] != \"RT\":\r\n",
    "                    #username\r\n",
    "                    user_id = data[\"user\"][\"screen_name\"]\r\n",
    "                    #location\r\n",
    "                    user_location = data[\"user\"][\"location\"]\r\n",
    "                    #Tweet ID\r\n",
    "                    tweet_id = data[\"id_str\"]\r\n",
    "                    #Hashtags\r\n",
    "                    hashtags = data[\"entities\"][\"hashtags\"]\r\n",
    "                    # Retweet?\r\n",
    "                    retweet = data[\"retweeted\"]\r\n",
    "                    #tweet text\r\n",
    "                    if \"extended_tweet\" in data:\r\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\r\n",
    "                    else:\r\n",
    "                        text = data[\"text\"]\r\n",
    "                    rows.append((user_id, user_location, tweet_id, hashtags, text))\r\n",
    "        pd.options.display.max_colwidth = 500\r\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_id\", \"user_location\", \"tweet_id\", \"hashtags\", \"text\"])\r\n",
    "        datas.to_feather(save_dir)\r\n",
    "        print(\"DONE\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../jsons\\2020-03-25.json\n",
      "DONE\n",
      "../jsons\\2020-03-26.json\n",
      "DONE\n",
      "../jsons\\2020-03-27.json\n",
      "DONE\n",
      "../jsons\\2020-03-28.json\n",
      "DONE\n",
      "../jsons\\2020-03-29.json\n",
      "DONE\n",
      "../jsons\\2020-03-30.json\n",
      "DONE\n",
      "../jsons\\2020-03-31.json\n",
      "DONE\n",
      "../jsons\\2020-04-01.json\n",
      "DONE\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following creates individual files including and excluding pronouns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Checks for pronouns\r\n",
    "def whole_word_checker(pronouns):\r\n",
    "    return re.compile(r'\\b({0})\\b'.format(pronouns), flags=re.IGNORECASE).search"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def pronoun_checker(text):\r\n",
    "    vocab = [\"i\", \"me\", \"you\", \"we\", \"they\", \"my\", \"us\", \"youâ€™re\", \"I'm\", \"we're\", \"they're\", \"you've\", \"they've\", \"you'll\", \"i'll\", \"we'll\", \"they'll\"]\r\n",
    "    check = []\r\n",
    "    for i in vocab:\r\n",
    "        check.append(whole_word_checker(i)(text.lower()))\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    for i in check:\r\n",
    "        if i == None:\r\n",
    "            count+=1\r\n",
    "    \r\n",
    "    if count < len(vocab):\r\n",
    "        return True\r\n",
    "    else:\r\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The following process finds all tweets and checks if they contain pronouns.\r\n",
    "# If the user wants pronouns, change the 'pronouns' variable at the top of the cell to True.\r\n",
    "# If the user does not want pronouns, change the 'pronouns' variable at the top of the cell to False"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Pronoun False or True?\r\n",
    "pronouns = False\r\n",
    "\r\n",
    "if pronouns == True:\r\n",
    "    additional_dir = '-with-pronouns.ftr'\r\n",
    "else:\r\n",
    "    additional_dir = '-without-pronouns.ftr'\r\n",
    "print(additional_dir)\r\n",
    "json_dir = '../jsons'\r\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\r\n",
    "json_pattern = os.path.join(json_dir, '*.json')\r\n",
    "\r\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\r\n",
    "file_list = glob.glob(json_pattern)\r\n",
    "\r\n",
    "# Opens the JSON file\r\n",
    "for file in file_list:\r\n",
    "    print(file)\r\n",
    "    rows = []\r\n",
    "    links = []\r\n",
    "    count=0\r\n",
    "\r\n",
    "    with open(file, 'r') as f:\r\n",
    "        # directory to save data to\r\n",
    "        save_dir = file.replace('.json', additional_dir)\r\n",
    "\r\n",
    "        for line in f:\r\n",
    "            if count > 10000:\r\n",
    "                break\r\n",
    "            count = 0\r\n",
    "            data = json.loads(line)\r\n",
    "\r\n",
    "            if count == 0:\r\n",
    "                # Retweet?\r\n",
    "                if data[\"text\"][:2] != \"RT\":\r\n",
    "                    #username\r\n",
    "                    user_id = data[\"user\"][\"screen_name\"]\r\n",
    "                    #location\r\n",
    "                    user_location = data[\"user\"][\"location\"]\r\n",
    "                    #Tweet ID\r\n",
    "                    tweet_id = data[\"id_str\"]\r\n",
    "                    #Hashtags\r\n",
    "                    hashtags = data[\"entities\"][\"hashtags\"]\r\n",
    "                    # Retweet?\r\n",
    "                    retweet = data[\"retweeted\"]\r\n",
    "                    #tweet text\r\n",
    "                    if \"extended_tweet\" in data:\r\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\r\n",
    "                        if pronoun_checker(text) == pronouns:\r\n",
    "                            rows.append((user_id, user_location, tweet_id, hashtags, text))\r\n",
    "                    else:\r\n",
    "                        text = data[\"text\"]\r\n",
    "                        if pronoun_checker(text) == pronouns:\r\n",
    "                            rows.append((user_id, user_location, tweet_id, hashtags, text))\r\n",
    "                            \r\n",
    "        pd.options.display.max_colwidth = 500\r\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_id\", \"user_location\", \"tweet_id\", \"hashtags\", \"text\"])\r\n",
    "        datas.to_feather(save_dir)\r\n",
    "        print(\"DONE\", save_dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-without-pronouns.ftr\n",
      "../jsons\\2020-03-25.json\n",
      "DONE ../jsons\\2020-03-25-without-pronouns.ftr\n",
      "../jsons\\2020-03-26.json\n",
      "DONE ../jsons\\2020-03-26-without-pronouns.ftr\n",
      "../jsons\\2020-03-27.json\n",
      "DONE ../jsons\\2020-03-27-without-pronouns.ftr\n",
      "../jsons\\2020-03-28.json\n",
      "DONE ../jsons\\2020-03-28-without-pronouns.ftr\n",
      "../jsons\\2020-03-29.json\n",
      "DONE ../jsons\\2020-03-29-without-pronouns.ftr\n",
      "../jsons\\2020-03-30.json\n",
      "DONE ../jsons\\2020-03-30-without-pronouns.ftr\n",
      "../jsons\\2020-03-31.json\n",
      "DONE ../jsons\\2020-03-31-without-pronouns.ftr\n",
      "../jsons\\2020-04-01.json\n",
      "DONE ../jsons\\2020-04-01-without-pronouns.ftr\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}