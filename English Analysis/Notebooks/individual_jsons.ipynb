{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, glob\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This nb contains the processes which save individual jsons to individual ftrs.\n",
    "# This nb also contains the processes which find whether a tweets contains...\n",
    "# ...or does not contain pronouns and saves this to their respective seperate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../jsons\\2020-04-01.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list[7:8]:\n",
    "    print(file)\n",
    "    rows = []\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to save data to\n",
    "        save_dir = file.replace('.json', '.ftr')\n",
    "\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                #username\n",
    "                user_id = data[\"user\"][\"screen_name\"]\n",
    "                #location\n",
    "                user_location = data[\"user\"][\"location\"]\n",
    "                #Tweet ID\n",
    "                tweet_id = data[\"id_str\"]\n",
    "                #Hashtags\n",
    "                hashtags = data[\"entities\"][\"hashtags\"]\n",
    "                # Retweet?\n",
    "                retweet = data[\"retweeted\"]\n",
    "                #tweet text\n",
    "                if \"extended_tweet\" in data:\n",
    "                    text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                else:\n",
    "                    text = data[\"text\"]\n",
    "                rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "\n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\", \"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates individual files including and excluding pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for pronouns\n",
    "def whole_word_checker(pronouns):\n",
    "    return re.compile(r'\\b({0})\\b'.format(pronouns), flags=re.IGNORECASE).search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun_checker(text):\n",
    "    vocab = [\"I\", \"They\", \"We\", \"My\", \"Us\", \"You\", \"Me\"]\n",
    "    check = []\n",
    "    for i in vocab:\n",
    "        check.append(whole_word_checker(i)(text.lower()))\n",
    "\n",
    "    count = 0\n",
    "    for i in check:\n",
    "        if i == None:\n",
    "            count+=1\n",
    "    \n",
    "    if count < len(vocab):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following process finds all tweets and checks if they contain pronouns.\n",
    "# If the user wants pronouns, change the 'pronouns' variable at the top of the cell to True.\n",
    "# If the user does not want pronouns, change the 'pronouns' variable at the top of the cell to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from prepro.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\2020-03-25.json\n",
      "../jsons\\2020-03-25-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-25-pre-me-pronoun.ftr\n",
      "\\2020-03-26.json\n",
      "../jsons\\2020-03-26-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-26-pre-me-pronoun.ftr\n",
      "\\2020-03-27.json\n",
      "../jsons\\2020-03-27-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-27-pre-me-pronoun.ftr\n",
      "\\2020-03-28.json\n",
      "../jsons\\2020-03-28-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-28-pre-me-pronoun.ftr\n",
      "\\2020-03-29.json\n",
      "../jsons\\2020-03-29-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-29-pre-me-pronoun.ftr\n",
      "\\2020-03-30.json\n",
      "../jsons\\2020-03-30-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-30-pre-me-pronoun.ftr\n",
      "\\2020-03-31.json\n",
      "../jsons\\2020-03-31-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-03-31-pre-me-pronoun.ftr\n",
      "\\2020-04-01.json\n",
      "../jsons\\2020-04-01-pre-me-pronoun.ftr\n",
      "DONE ../jsons\\2020-04-01-pre-me-pronoun.ftr\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_location</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nerdsville USA</td>\n",
       "      <td>1245501185576050691</td>\n",
       "      <td>[lolatmikepeters, im, one, theory, me, think, dnc, want, shittiest, runner, they, trump, another, year, secretly, make, bunch, money, bitching, election, rigged, whole, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>1245501176331882502</td>\n",
       "      <td>[defensivelb, dont, mind, me, im, spreading, georgie, kelly, usual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>1245501165732683778</td>\n",
       "      <td>[onyxperidot, aoverk, bruh, dont, tell, me, you, believe, andrenochrome, shit, donald, trump, really, good, guy, thats, he, separating, child, border, liberal, elite, dont, drink, blood, real, people, believe, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she/her, 20</td>\n",
       "      <td>1245501158149382144</td>\n",
       "      <td>[let, me, put, tinfoil, hat, say, video, used, pro, govt, distract, fact, theyre, letting, healthcare, industry, thing, passively, restrict, poor, people, access, testing, kit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>1245501130013982720</td>\n",
       "      <td>[realcandaceo, i, dont, know, you, remind, me, alex, jones, amp, sandy, hook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>None</td>\n",
       "      <td>1245139047149760513</td>\n",
       "      <td>[sophnarseven, hundred, fortyseven, make, sense, me, head, member, communist, party, ethiopia, also, covered, cholera, epidemic, getting, spot, doesnt, care, health, they, outlet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>None</td>\n",
       "      <td>1245139001557897216</td>\n",
       "      <td>[stillgray, yes, new, normal, fcucking, pleb, like, memeanwhile, super, rich, hollyweird, millionaire, politician, millionaire, peddler, carry, life, wont, change, one, iota]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>Accra, Ghana</td>\n",
       "      <td>1245138908687597569</td>\n",
       "      <td>[theishmaelarmah, unfortunately, vodafone, helped, me, today, unreliable, internet, enabling, me, read]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>None</td>\n",
       "      <td>1245138870070640641</td>\n",
       "      <td>[spirit, tell, me, lie, mixed, false]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>1245138811362906112</td>\n",
       "      <td>[jalicat, speakerpelosi, they, purpose, they, wanted, many, infected, possible, call, me, theorist, i, dont, care, they, wanted, many, people, infected, possible, hospital, would, overrun, death, toll, astronomical]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_location             tweet_id  \\\n",
       "0     Nerdsville USA  1245501185576050691   \n",
       "1               None  1245501176331882502   \n",
       "2               None  1245501165732683778   \n",
       "3        she/her, 20  1245501158149382144   \n",
       "4               None  1245501130013982720   \n",
       "...              ...                  ...   \n",
       "4737            None  1245139047149760513   \n",
       "4738            None  1245139001557897216   \n",
       "4739    Accra, Ghana  1245138908687597569   \n",
       "4740            None  1245138870070640641   \n",
       "4741    Florida, USA  1245138811362906112   \n",
       "\n",
       "                                                                                                                                                                                                                         text  \n",
       "0                                              [lolatmikepeters, im, one, theory, me, think, dnc, want, shittiest, runner, they, trump, another, year, secretly, make, bunch, money, bitching, election, rigged, whole, time]  \n",
       "1                                                                                                                                                         [defensivelb, dont, mind, me, im, spreading, georgie, kelly, usual]  \n",
       "2      [onyxperidot, aoverk, bruh, dont, tell, me, you, believe, andrenochrome, shit, donald, trump, really, good, guy, thats, he, separating, child, border, liberal, elite, dont, drink, blood, real, people, believe, lol]  \n",
       "3                                            [let, me, put, tinfoil, hat, say, video, used, pro, govt, distract, fact, theyre, letting, healthcare, industry, thing, passively, restrict, poor, people, access, testing, kit]  \n",
       "4                                                                                                                                               [realcandaceo, i, dont, know, you, remind, me, alex, jones, amp, sandy, hook]  \n",
       "...                                                                                                                                                                                                                       ...  \n",
       "4737                                      [sophnarseven, hundred, fortyseven, make, sense, me, head, member, communist, party, ethiopia, also, covered, cholera, epidemic, getting, spot, doesnt, care, health, they, outlet]  \n",
       "4738                                           [stillgray, yes, new, normal, fcucking, pleb, like, memeanwhile, super, rich, hollyweird, millionaire, politician, millionaire, peddler, carry, life, wont, change, one, iota]  \n",
       "4739                                                                                                                  [theishmaelarmah, unfortunately, vodafone, helped, me, today, unreliable, internet, enabling, me, read]  \n",
       "4740                                                                                                                                                                                    [spirit, tell, me, lie, mixed, false]  \n",
       "4741  [jalicat, speakerpelosi, they, purpose, they, wanted, many, infected, possible, call, me, theorist, i, dont, care, they, wanted, many, people, infected, possible, hospital, would, overrun, death, toll, astronomical]  \n",
       "\n",
       "[4742 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "if pronouns == True:\n",
    "    additional_dir = '-pre-me-pronoun.ftr'\n",
    "else:\n",
    "    additional_dir = '-without-me-pronouns.ftr'\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "    rows = []\n",
    "    links = []\n",
    "    count=0\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to save data to\n",
    "        save_dir = file.replace('.json', additional_dir)\n",
    "        print(save_dir)\n",
    "\n",
    "        for line in f:\n",
    "            if count > 10000:\n",
    "                break\n",
    "            count = 0\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if count == 0:\n",
    "                # Retweet?\n",
    "                if data[\"text\"][:2] != \"RT\":\n",
    "                    #username\n",
    "                    user_id = data[\"user\"][\"screen_name\"]\n",
    "                    #location\n",
    "                    user_location = data[\"user\"][\"location\"]\n",
    "                    #Tweet ID\n",
    "                    tweet_id = data[\"id_str\"]\n",
    "                    #Hashtags\n",
    "                    hashtags = data[\"entities\"][\"hashtags\"]\n",
    "                    # Retweet?\n",
    "                    retweet = data[\"retweeted\"]\n",
    "                    #tweet text\n",
    "                    if \"extended_tweet\" in data:\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "                    else:\n",
    "                        text = data[\"text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, prepro.preprocess(text)))\n",
    "                            \n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\", \"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\", save_dir)\n",
    "\n",
    "display(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "if pronouns == True:\n",
    "    additional_dir = '-youre-pronoun.ftr'\n",
    "else:\n",
    "    additional_dir = '-without-me-pronouns.ftr'\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "    rows = []\n",
    "    links = []\n",
    "    count=0\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        # directory to save data to\n",
    "        save_dir = file.replace('.json', additional_dir)\n",
    "        print(save_dir)\n",
    "\n",
    "        for line in f:\n",
    "            if count > 10000:\n",
    "                break\n",
    "            count = 0\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if count == 0:\n",
    "                # Retweet?\n",
    "                if data[\"text\"][:2] != \"RT\":\n",
    "                    #username\n",
    "                    user_id = data[\"user\"][\"screen_name\"]\n",
    "                    #location\n",
    "                    user_location = data[\"user\"][\"location\"]\n",
    "                    #Tweet ID\n",
    "                    tweet_id = data[\"id_str\"]\n",
    "                    #Hashtags\n",
    "                    hashtags = data[\"entities\"][\"hashtags\"]\n",
    "                    # Retweet?\n",
    "                    retweet = data[\"retweeted\"]\n",
    "                    #tweet text\n",
    "                    if \"extended_tweet\" in data:\n",
    "                        text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, text))\n",
    "                    else:\n",
    "                        text = data[\"text\"]\n",
    "                        if pronoun_checker(text) == pronouns:\n",
    "                            rows.append((user_location, tweet_id, text))\n",
    "                            \n",
    "        pd.options.display.max_colwidth = 500\n",
    "        datas = pd.DataFrame(rows, columns=[\"user_location\", \"tweet_id\", \"text\"])\n",
    "        datas.to_feather(save_dir)\n",
    "        print(\"DONE\", save_dir)\n",
    "        break\n",
    "\n",
    "display(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3082615\n"
     ]
    }
   ],
   "source": [
    "# Count all tweets\n",
    "\n",
    "count=0\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004281\n"
     ]
    }
   ],
   "source": [
    "# Count all retweets\n",
    "\n",
    "count=0\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        dic = {}\n",
    "        data = json.load(f)\n",
    "        dic\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            if data[\"text\"][:2] == \"RT\":\n",
    "                count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNT TWEETS CONTAINING PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\2020-03-25.json\n",
      "DONE\n",
      "\\2020-03-26.json\n",
      "DONE\n",
      "\\2020-03-27.json\n",
      "DONE\n",
      "\\2020-03-28.json\n",
      "DONE\n",
      "\\2020-03-29.json\n",
      "DONE\n",
      "\\2020-03-30.json\n",
      "DONE\n",
      "\\2020-03-31.json\n",
      "DONE\n",
      "\\2020-04-01.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "all_count = 0\n",
    "tweet_count=0\n",
    "pronoun_tweet_count = 0\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            all_count+=1\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                if \"extended_tweet\" in data:\n",
    "                    text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                    tweet_count+=1\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweet_count += 1\n",
    "                else:\n",
    "                    text = data[\"text\"]\n",
    "                    tweet_count+=1\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweet_count += 1                        \n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078334 456501\n"
     ]
    }
   ],
   "source": [
    "print(tweet_count, pronoun_tweet_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\2020-03-25.json\n",
      "DONE\n",
      "\\2020-03-26.json\n",
      "DONE\n",
      "\\2020-03-27.json\n",
      "DONE\n",
      "\\2020-03-28.json\n",
      "DONE\n",
      "\\2020-03-29.json\n",
      "DONE\n",
      "\\2020-03-30.json\n",
      "DONE\n",
      "\\2020-03-31.json\n",
      "DONE\n",
      "\\2020-04-01.json\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Pronoun False or True?\n",
    "pronouns = True\n",
    "\n",
    "json_dir = '../jsons'\n",
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "\n",
    "# Uses Glob with the json pattern variable to place all the json files and their directories in a list\n",
    "file_list = glob.glob(json_pattern)\n",
    "all_tweets= 0\n",
    "pronoun_tweets=0\n",
    "all_tweets_word_count = 0\n",
    "pronoun_tweet_word_count = 0\n",
    "\n",
    "# Opens the JSON file\n",
    "for file in file_list:\n",
    "    print(file[8:])\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            # Retweet?\n",
    "            if data[\"text\"][:2] != \"RT\":\n",
    "                if \"extended_tweet\" in data:\n",
    "                    text = data[\"extended_tweet\"][\"full_text\"]\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweets+=1\n",
    "                        for i in text:\n",
    "                            pronoun_tweet_word_count += 1\n",
    "                    else:\n",
    "                        all_tweets+=1\n",
    "                        for i in text:\n",
    "                            all_tweets_word_count+=1\n",
    "\n",
    "                else:\n",
    "                    text = data[\"text\"]\n",
    "                    if pronoun_checker(text) == True:\n",
    "                        pronoun_tweets+=1\n",
    "                        for i in text:\n",
    "                            pronoun_tweet_word_count += 1     \n",
    "                    else:\n",
    "                        all_tweets+=1\n",
    "                        for i in text:\n",
    "                            all_tweets_word_count+=1               \n",
    "        print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.74640458129434\n",
      "\n",
      "\n",
      "202.67002481922273\n"
     ]
    }
   ],
   "source": [
    "print(all_tweets_word_count/all_tweets)\n",
    "print('\\n')\n",
    "\n",
    "#pronoun tweet wordcount = 202.67002481922273\n",
    "print(pronoun_tweet_word_count/pronoun_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "188.708960111807\n",
    "\n",
    "\n",
    "202.67002481922273"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
