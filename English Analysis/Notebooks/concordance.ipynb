{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "from tqdm import tqdm\r\n",
    "import os\r\n",
    "import re\r\n",
    "from num2words import num2words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre Processing before concordance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess(text):\r\n",
    "    text = text.lower()\r\n",
    "    text = remove_apostrophes(text)\r\n",
    "    text = remove_punctuation(text)\r\n",
    "    text = remove_single_chars(text)\r\n",
    "    text = remove_links(text)\r\n",
    "    text = remove_search_terms(text)\r\n",
    "    text = numbers_to_words(text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def numbers_to_words(text):\r\n",
    "    num_list = re.findall(r'\\d+', text)\r\n",
    "    str_num_list = []\r\n",
    "\r\n",
    "    for num in num_list:\r\n",
    "            str_with_punct = num2words(num)\r\n",
    "            str_without_punct = re.sub('[^A-Za-z0-9 ]+', '', str_with_punct)\r\n",
    "            str_num_list.append(str_without_punct)\r\n",
    "    \r\n",
    "    for i, num in enumerate(num_list):\r\n",
    "        text = text.replace(str(num), str_num_list[i])\r\n",
    "  \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove apostrophes\r\n",
    "def remove_apostrophes(text):\r\n",
    "    text = text.replace(\"'\", \"\")\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_punctuation(text):\r\n",
    "    words = []\r\n",
    "    for word in text:\r\n",
    "        w = re.sub(r'([^\\w\\s\\d]|_)','',word)\r\n",
    "        words.append(w)\r\n",
    "    words = \"\".join(words)\r\n",
    "\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove Search Terms\r\n",
    "def remove_search_terms(stripped_string):\r\n",
    "    stripped_string = stripped_string.replace(\"conspiracy\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"propaganda\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"Trump\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fake\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"realDonald\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"misinformation\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fake news\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"disinformation\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"active measures\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"subversion\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"interference\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"influence\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"deep state\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fabrication\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"manipulate\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"deceive\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"useful idiots\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"mainstream media\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"populism\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"untrustworthy\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"made-up\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"bogus\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"inaccurate\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"doctored\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fact Checking\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu troll\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe troll\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european troll\", \"\").strip()\r\n",
    "    return stripped_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_single_chars(text):\r\n",
    "    txt = text.split()\r\n",
    "    updated_text = \"\"\r\n",
    "    for word in txt:\r\n",
    "        if len(word) > 1 or word.lower() == \"i\":\r\n",
    "            updated_text = updated_text + \" \" + word\r\n",
    "    return updated_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove Links\r\n",
    "def remove_links(text):\r\n",
    "    text = re.sub(r'http\\S+', '', text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_path = '../textFiles/EN-03-2020-No-Retweets.txt'\r\n",
    "tokens = \"\"\r\n",
    "\r\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\r\n",
    "    for i, line in enumerate(tqdm(f.readlines(), total=500000)):\r\n",
    "        tokens+=line\r\n",
    "        if i == 500000:\r\n",
    "            tokens = preprocess(tokens)\r\n",
    "            tokens = nltk.word_tokenize(tokens)\r\n",
    "            break\r\n",
    "\r\n",
    "text = nltk.Text(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def concordanceToFile(conc):\r\n",
    "    filename = '../textFiles/Pronoun/concordance_' + conc + '.txt'\r\n",
    "    conc_save = text.concordance_list(conc, lines=50000)\r\n",
    "    \r\n",
    "    with open(filename, 'w', encoding='utf8') as fin:\r\n",
    "        for i, line in enumerate(tqdm(conc_save, total=len(conc_save))):\r\n",
    "                fin.write(str(line).strip(\"ConcordanceLine(\") + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concordance\r\n",
    "Functions which run the concordance on a list of words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def custom_concordance(list_of_words):\r\n",
    "    for i in list_of_words:\r\n",
    "        concordanceToFile(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pronoun_concordance():\r\n",
    "    pronouns = [\"I\", \"me\", \"you\", \"we\", \"they\", \"my\", \"us\"]\r\n",
    "    for i in pronouns:\r\n",
    "        concordanceToFile(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def verb_concordance():\r\n",
    "    verbs = [\"have\", \"believe\", \"think\", \"know\", \"tell\", \"need\", \"want\"]\r\n",
    "    for i in verbs:\r\n",
    "        concordanceToFile(i)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}