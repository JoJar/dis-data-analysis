{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, glob, nltk\n",
    "import pandas as pd\n",
    "import feather\n",
    "from collections import Counter\n",
    "from nltk import bigrams, ngrams, trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['myself', 'i', 'im','my', 'you','me','we','youre','youve','weve','gon','na', 'nine','hundred','thousand','two','seven','eight','me', 'our', 'ours', 'ourselves', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "    \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself',\n",
    "    'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
    "    'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
    "    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "    'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're',\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
    "    \"won't\", 'wouldn', \"wouldn't\", \"fool\", \"ive\", \"dont\", \"youd\", \"would\", \"theyre\", \"they\", \"else\", \"youll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(n, path, filter_word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lt = [x[0] for x in os.walk(path)]\n",
    "\n",
    "    dict_pronoun_words = {}\n",
    "    bigrams_dict = {}\n",
    "\n",
    "    # Ngrams with 'creature' as a member\n",
    "    think_filter = lambda *w: 'think' not in w\n",
    "\n",
    "    for ftr_dir in lt[1:]:\n",
    "        # Gets pronoun from file name\n",
    "        pronoun = ftr_dir[21:]\n",
    "\n",
    "        # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "        ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "\n",
    "        file_list = glob.glob(ftr_pattern)\n",
    "        \n",
    "        for i in file_list:\n",
    "            bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "            flat_list = []\n",
    "            read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "            ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "            flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "            dict_pronoun_words[pronoun] = flat_list\n",
    "        \n",
    "        if n < 3:\n",
    "            # Bigrams\n",
    "            \n",
    "            for i in dict_pronoun_words:\n",
    "                tokens = dict_pronoun_words.get(i)\n",
    "                tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "                tokens= tokens.split()\n",
    "                tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "                finder = BigramCollocationFinder.from_words(tokens)\n",
    "                # only bigrams that appear 3+ times\n",
    "                finder.apply_freq_filter(3)\n",
    "                # Ngrams with 'creature' as a member\n",
    "                ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "                # only bigrams that contain the filter word\n",
    "                finder.apply_ngram_filter(ngram_word_filter)\n",
    "                #text_bigrams = bigrams(tokens)\n",
    "                bigrams_dict[i] = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "        if n == 3:\n",
    "            for i in dict_pronoun_words:\n",
    "                tokens = dict_pronoun_words.get(i)\n",
    "                tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "                tokens= tokens.split()\n",
    "                tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "                finder = TrigramCollocationFinder.from_words(tokens)\n",
    "                # only bigrams that appear 3+ times\n",
    "                finder.apply_freq_filter(3)\n",
    "                # Ngrams with 'creature' as a member\n",
    "                ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "                # only bigrams that contain the filter word\n",
    "                finder.apply_ngram_filter(ngram_word_filter)\n",
    "                #text_bigrams = bigrams(tokens)\n",
    "                bigrams_dict[i] = finder.nbest(trigram_measures.likelihood_ratio, 10)\n",
    "\n",
    "        else:\n",
    "            for i in dict_pronoun_words:\n",
    "                tokens = dict_pronoun_words.get(i)\n",
    "\n",
    "            list_of_ngrams = []\n",
    "            sixgrams = list_of_ngrams.append(ngrams(tokens, n))\n",
    "\n",
    "            return list_of_ngrams\n",
    "    if n <= 3:\n",
    "        return bigrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_dir = '../all-en-ftrs/'\n",
    "\n",
    "def get_ngrams_new(n, path, filter_word, flat_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    dict_pronoun_words = {}\n",
    "    bigrams_dict = {}\n",
    "    \n",
    "    if n < 3:\n",
    "        # Bigrams\n",
    "        for i in dict_pronoun_words:\n",
    "            tokens = dict_pronoun_words.get(i)\n",
    "            tokens = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "            tokens= tokens.split()\n",
    "            tokens = [wrd for wrd in tokens if wrd.lower() not in stop_words]\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "            # only bigrams that appear 3+ times\n",
    "            finder.apply_freq_filter(3)\n",
    "            # Ngrams with 'creature' as a member\n",
    "            ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "            # only bigrams that contain the filter word\n",
    "            finder.apply_ngram_filter(ngram_word_filter)\n",
    "            #text_bigrams = bigrams(tokens)\n",
    "            bigrams_dict[i] = finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "            \n",
    "    if n == 3:\n",
    "        flat_list = [wrd for wrd in flat_list if wrd.lower() not in stop_words]\n",
    "        finder = TrigramCollocationFinder.from_words(flat_list)\n",
    "\n",
    "        # only bigrams that appear 3+ times\n",
    "        finder.apply_freq_filter(3)\n",
    "        # Ngrams with 'creature' as a member\n",
    "        ngram_word_filter = lambda *w: filter_word not in w\n",
    "\n",
    "        # only bigrams that contain the filter word\n",
    "        finder.apply_ngram_filter(ngram_word_filter)\n",
    "        #text_bigrams = bigrams(tokens)\n",
    "        trigrams = finder.nbest(trigram_measures.likelihood_ratio, 20)\n",
    "\n",
    "    if n <= 3:\n",
    "        return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "file_list = glob.glob(ftr_pattern)\n",
    "flat_list = []\n",
    "for i in file_list:  \n",
    "    read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "    ls = read_ftr[\"text\"].tolist()\n",
    "    \n",
    "    flat_list += [item for sublist in ls for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_beieve = get_ngrams_new(3, '../ftrs/Pronouns/', 'believe', flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('believe', 'fact', 'checking'), ('fact', 'checking', 'believe'), ('via', 'youtube', 'believe'), ('believe', 'social', 'medium'), ('social', 'medium', 'believe'), ('stop', 'spreading', 'believe'), ('believe', 'new', 'york'), ('press', 'conference', 'believe'), ('believe', 'cnn', 'msnbc'), ('cnn', 'msnbc', 'believe'), ('believe', 'corona', 'virus'), ('corona', 'virus', 'believe'), ('believe', 'president', 'trump'), ('president', 'trump', 'believe'), ('right', 'wing', 'believe'), ('believe', 'right', 'wing'), ('believe', 'donald', 'trump'), ('united', 'state', 'believe'), ('believe', 'united', 'state'), ('believe', 'dr', 'fauci')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_dic_beieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_think = get_ngrams_new(3, '../ftrs/Pronouns/', 'think', flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('think', 'fact', 'checking'), ('fact', 'checking', 'think'), ('via', 'youtube', 'think'), ('think', 'social', 'medium'), ('social', 'medium', 'think'), ('stop', 'spreading', 'think'), ('think', 'stop', 'spreading'), ('think', 'new', 'york'), ('think', 'press', 'conference'), ('press', 'conference', 'think'), ('think', 'corona', 'virus'), ('think', 'cnn', 'msnbc'), ('cnn', 'msnbc', 'think'), ('corona', 'virus', 'think'), ('white', 'house', 'think'), ('think', 'white', 'house'), ('think', 'president', 'trump'), ('president', 'trump', 'think'), ('think', 'right', 'wing'), ('right', 'wing', 'think')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_dic_think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_know = get_ngrams_new(3, '../ftrs/Pronouns/', 'know', flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('know', 'fact', 'checking'), ('fact', 'checking', 'know'), ('know', 'via', 'youtube'), ('via', 'youtube', 'know'), ('know', 'social', 'medium'), ('social', 'medium', 'know'), ('know', 'stop', 'spreading'), ('stop', 'spreading', 'know'), ('new', 'york', 'know'), ('know', 'new', 'york'), ('press', 'conference', 'know'), ('know', 'cnn', 'msnbc'), ('cnn', 'msnbc', 'know'), ('know', 'corona', 'virus'), ('corona', 'virus', 'know'), ('white', 'house', 'know'), ('know', 'white', 'house'), ('know', 'president', 'trump'), ('president', 'trump', 'know'), ('know', 'right', 'wing')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_dic_know)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_dic = get_ngrams(2, '../all-en-ftrs/Words', \"think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_measures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9448/146503503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrigrams_dic_think\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../all-en-ftrs/Words'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"think\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9448/196071309.py\u001b[0m in \u001b[0;36mget_ngrams\u001b[1;34m(n, path, filter_word)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_ngram_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_word_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;31m#text_bigrams = bigrams(tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mbigrams_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrigram_measures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlikelihood_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trigram_measures' is not defined"
     ]
    }
   ],
   "source": [
    "trigrams_dic_think = get_ngrams(3, '../all-en-ftrs/Words', \"think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      " [('think', 'president', 'trump'), ('think', 'trump', 'supporter'), ('cant', 'help', 'think'), ('people', 'still', 'think'), ('think', 'thats', 'great'), ('think', 'generally', 'dumb'), ('think', '𝐜𝐫𝐢𝐭𝐢𝐜𝐚𝐥𝐥𝐲', 'gullible'), ('acted', 'differently', 'think'), ('parent', 'think', '𝐜𝐫𝐢𝐭𝐢𝐜𝐚𝐥𝐥𝐲'), ('real', 'think', 'generally')]\n",
      "Me \n",
      " [('know', 'people', 'think')]\n",
      "My \n",
      " [('father', 'think', 'nothing'), ('think', 'nothing', 'flu'), ('elderly', 'father', 'think')]\n",
      "They \n",
      " [('parent', 'think', '𝐜𝐫𝐢𝐭𝐢𝐜𝐚𝐥𝐥𝐲'), ('although', 'think', 'great'), ('think', '𝐜𝐫𝐢𝐭𝐢𝐜𝐚𝐥𝐥𝐲', 'gullible'), ('think', 'theyve', 'made'), ('people', 'still', 'think'), ('dja', 'ever', 'think'), ('prank', 'parent', 'think'), ('think', 'great', 'nation'), ('one', 'people', 'think'), ('ever', 'think', 'theyve')]\n",
      "Us \n",
      " [('tell', 'u', 'think'), ('think', 'president', 'trump'), ('think', 'trump', 'supporter'), ('people', 'still', 'think')]\n",
      "We \n",
      " [('nope', 'vigilant', 'think'), ('vigilant', 'think', 'sir'), ('think', 'sir', 'realdonaldtrump'), ('really', 'think', 'stupid')]\n",
      "You \n",
      " [('many', 'people', 'think'), ('stop', 'spread', 'think'), ('nope', 'vigilant', 'think'), ('vigilant', 'think', 'sir'), ('really', 'think', 'trump'), ('think', 'outside', 'box'), ('think', 'sir', 'realdonaldtrump'), ('honestly', 'think', 'american'), ('people', 'still', 'think'), ('programming', 'think', 'carefully')]\n"
     ]
    }
   ],
   "source": [
    "for i in trigrams_dic_think:\n",
    "    print(i, \"\\n\", trigrams_dic_think.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dic_believe = get_ngrams(3, '../all-en-ftrs/Words', \"believe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      " [('believe', 'president', 'trump'), ('saver', 'europe', 'believe'), ('still', 'cant', 'believe'), ('cant', 'believe', 'people'), ('cant', 'believe', 'even'), ('europe', 'believe', 'virus'), ('believe', 'anything', 'say'), ('believe', 'virus', 'made'), ('want', 'believe', 'theory'), ('starting', 'believe', 'theory')]\n",
      "Me \n",
      " []\n",
      "My \n",
      " [('people', 'believe', 'theory')]\n",
      "They \n",
      " [('cant', 'believe', 'anything'), ('believe', 'everything', 'say'), ('want', 'u', 'believe'), ('believe', 'everything', 'tell'), ('saver', 'europe', 'believe'), ('believe', 'everything', 'see'), ('believe', 'u', 'intelligence'), ('believe', 'word', 'say'), ('care', 'people', 'believe'), ('people', 'believe', 'theory')]\n",
      "Us \n",
      " [('saver', 'europe', 'believe'), ('believe', 'u', 'intelligence'), ('europe', 'believe', 'virus'), ('want', 'u', 'believe'), ('believe', 'virus', 'made'), ('believe', 'virus', 'create'), ('people', 'believe', 'virus'), ('believe', 'black', 'plaguert'), ('believe', 'everything', 'tell'), ('want', 'believe', 'worst')]\n",
      "We \n",
      " [('supposed', 'believe', 'everything'), ('believe', 'eye', 'ear'), ('still', 'believe', 'death'), ('american', 'still', 'believe'), ('people', 'believe', 'theory'), ('believe', 'death', 'lie')]\n",
      "You \n",
      " [('believe', 'people', 'like'), ('cant', 'believe', 'anything'), ('go', 'ahead', 'believe'), ('believe', 'anything', 'say'), ('people', 'believe', 'anything'), ('believe', 'anything', 'coming'), ('believe', 'anything', 'read'), ('believe', 'everything', 'hear'), ('believe', 'everything', 'see'), ('believe', 'everything', 'read')]\n"
     ]
    }
   ],
   "source": [
    "for i in s_dic_believe:\n",
    "    print(i, \"\\n\", s_dic_believe.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_dic_know = get_ngrams(3, '../all-en-ftrs/Words', \"know\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      " [('know', 'sound', 'like'), ('know', 'april', 'day'), ('april', 'day', 'know'), ('know', 'whats', 'going'), ('really', 'want', 'know'), ('miraculously', 'know', 'whats'), ('know', 'whats', 'real'), ('know', 'whats', 'true'), ('id', 'like', 'know'), ('know', 'holded', 'wuhan')]\n",
      "Me \n",
      " [('please', 'let', 'know'), ('thanks', 'letting', 'know'), ('know', 'people', 'think')]\n",
      "My \n",
      " []\n",
      "They \n",
      " [('know', 'many', 'people'), ('want', 'know', 'truth'), ('wouldnt', 'even', 'know'), ('someone', 'know', 'motive'), ('know', 'true', 'hid'), ('know', 'motive', 'ever'), ('wide', 'someone', 'know'), ('docked', 'know', 'brazen'), ('story', 'know', 'true'), ('people', 'know', 'truth')]\n",
      "Us \n",
      " [('let', 'u', 'know'), ('want', 'u', 'know'), ('know', 'whats', 'going'), ('none', 'u', 'know'), ('many', 'u', 'know'), ('letting', 'u', 'know'), ('know', 'everything', 'american'), ('trump', 'know', 'everything'), ('virus', 'trump', 'know')]\n",
      "We \n",
      " [('know', 'whats', 'coming'), ('police', 'personnel', 'know'), ('need', 'know', 'whats'), ('know', 'whats', 'going'), ('know', 'world', 'health'), ('know', 'whats', 'true'), ('want', 'know', 'whats'), ('people', 'need', 'know'), ('joenbc', 'know', 'world'), ('know', 'want', 'pathologically')]\n",
      "You \n",
      " [('know', 'stop', 'spreading'), ('stop', 'spreading', 'know'), ('sound', 'like', 'know'), ('know', 'may', 'legal'), ('well', 'know', 'may'), ('know', 'whats', 'coming'), ('need', 'know', 'whats'), ('get', 'well', 'know'), ('police', 'personnel', 'know'), ('make', 'sure', 'know')]\n"
     ]
    }
   ],
   "source": [
    "for i in trigrams_dic_know:\n",
    "    print(i, \"\\n\", trigrams_dic_know.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped_You.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "I_corpusdir = '../textFiles/Pronoun/Large/Stripped/I'\n",
    "I_corpus = PlaintextCorpusReader(I_corpusdir, '.*')\n",
    "\n",
    "you_corpusdir = '../textFiles/Pronoun/Large/Stripped/you'\n",
    "you_corpus = PlaintextCorpusReader(you_corpusdir, '.*')\n",
    "\n",
    "me_corpusdir = '../textFiles/Pronoun/Large/Stripped/me'\n",
    "me_corpus = PlaintextCorpusReader(me_corpusdir, '.*')\n",
    "\n",
    "my_corpusdir = '../textFiles/Pronoun/Large/Stripped/my'\n",
    "my_corpus = PlaintextCorpusReader(my_corpusdir, '.*')\n",
    "\n",
    "they_corpusdir = '../textFiles/Pronoun/Large/Stripped/they'\n",
    "they_corpus = PlaintextCorpusReader(they_corpusdir, '.*')\n",
    "\n",
    "us_corpusdir = '../textFiles/Pronoun/Large/Stripped/us'\n",
    "us_corpus = PlaintextCorpusReader(us_corpusdir, '.*')\n",
    "\n",
    "we_corpusdir = '../textFiles/Pronoun/Large/Stripped/we'\n",
    "we_corpus = PlaintextCorpusReader(we_corpusdir, '.*')\n",
    "\n",
    "\n",
    "for infile in sorted(you_corpus.fileids()):\n",
    "    print(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'I_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21568/1857826742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m## Bigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mI_finder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# only bigrams that appear 3+ times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mI_finder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'I_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'creature' as a member\n",
    "creature_filter = lambda *w: 'i' not in w\n",
    "\n",
    "## Bigrams\n",
    "I_finder = BigramCollocationFinder.from_words(I_corpus.words(I_corpus.fileids()))\n",
    "# only bigrams that appear 3+ times\n",
    "I_finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "I_finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(I_finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(corpus):\n",
    "    stop_words = ['myself', 'our', 'ours', 'ourselves', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "    \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself',\n",
    "    'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
    "    'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n",
    "    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "    'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're',\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
    "    \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    \n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    \n",
    "    # Bigrams\n",
    "    finder = BigramCollocationFinder.from_words(corpus.words(corpus.fileids()))\n",
    "\n",
    "    # only bigrams that appear 3+ times\n",
    "    finder.apply_freq_filter(3)\n",
    "    finder = [tup for tup in finder if not False in [False for wrd in tup if wrd.lower() in stop_words]]\n",
    "\n",
    "    # only bigrams that contain 'creature'\n",
    "    # finder.apply_ngram_filter(creature_filter)\n",
    "\n",
    "    # return the 10 n-grams with the highest PMI\n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PlaintextCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3460/954483110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m## Bigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mI_finder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# only bigrams that appear 3+ times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mI_finder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\collocations.py\u001b[0m in \u001b[0;36mfrom_words\u001b[1;34m(cls, words, window_size)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Specify window_size at least 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mwindow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m             \u001b[0mw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mngrams\u001b[1;34m(sequence, n, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \"\"\"\n\u001b[1;32m--> 689\u001b[1;33m     \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;31m# Creates the sliding window, of n no. of items.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[1;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \"\"\"\n\u001b[1;32m--> 642\u001b[1;33m     \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpad_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_pad_symbol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PlaintextCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'creature' as a member\n",
    "think_filter = lambda *w: 'think' not in w\n",
    "\n",
    "## Bigrams\n",
    "I_finder = BigramCollocationFinder.from_words(I_corpus)\n",
    "# only bigrams that appear 3+ times\n",
    "I_finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'creature'\n",
    "I_finder.apply_ngram_filter(creature_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(I_finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \n",
      " [('good', 'person'), ('things', 'fit'), ('im', 'good'), ('im', 'not'), ('some', 'things'), ('on', 'hulu'), ('effect', 'on'), ('person', 'some'), ('fit', 'but'), ('hulu', 'im')]\n",
      "me \n",
      " [('this', 'is'), ('want', 'to'), ('i', 'am'), ('lamestream', 'media'), ('yours', 'astound'), ('sounds', 'like'), ('if', 'you'), ('rofessional', 'tweets'), ('please', 'tell'), ('trying', 'to')]\n",
      "you \n",
      " [('this', 'is'), ('need', 'to'), ('want', 'to'), ('shame', 'on'), ('fact', 'checking'), ('going', 'to'), ('trying', 'to'), ('she', 'does'), ('gon', 'na'), ('talking', 'about')]\n",
      "my \n",
      " [('angel', 'read'), ('expose', 'them'), ('an', 'angel'), ('read', 'twitter'), ('they', 'say'), ('them', 'they'), ('say', 'im'), ('im', 'an'), ('twitter', 'and'), ('and', 'facebook')]\n",
      "they \n",
      " [('expose', 'them'), ('say', 'im'), ('not', 'expose'), ('them', 'say'), ('used', 'to'), ('angel', 'read'), ('and', 'used'), ('third', 'one'), ('an', 'angel'), ('my', 'twitter')]\n",
      "us \n",
      " [('in', 'the'), ('scientific', 'dat'), ('game', 'escalates'), ('escalates', 'between'), ('trust', 'scientific'), ('mullah', 'regim'), ('tyrant', 'mullah'), ('days', 'ahead'), ('blame', 'game'), ('ahead', 'each')]\n",
      "we \n",
      " [('as', 'well'), ('they', 'were'), ('need', 'to'), ('this', 'is'), ('circa', 'wwii'), ('nazi', 'germany'), ('observed', 'celebrating'), ('germany', 'circa'), ('cents', 'ie'), ('fourth', 'orange')]\n"
     ]
    }
   ],
   "source": [
    "#I_finder.nbest(bigram_measures.likelihood_ratio, 10)\n",
    "vocab = [\"I\", \"me\", \"you\", \"my\", \"they\", \"us\", \"we\"]\n",
    "ls = [I_corpus,\n",
    "me_corpus,\n",
    "you_corpus,\n",
    "my_corpus,\n",
    "they_corpus,\n",
    "us_corpus,\n",
    "we_corpus]\n",
    "\n",
    "think_filter = lambda *w: 'think' not in w\n",
    "\n",
    "for i,j in enumerate(ls):\n",
    "    print(vocab[i], \"\\n\", find_bigrams(j))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
