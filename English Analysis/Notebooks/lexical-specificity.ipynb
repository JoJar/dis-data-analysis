{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import feather\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import lexical_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file implements the code found in lexical_specificity.py provided by the CSRI team.\n",
    "# It gets the lexical specificity for each corpus of pronoun tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "                P(X=i)(F−i)(t−i)\n",
    "P(X=i+1) =      ________________\n",
    "                (i+1)(T−F−t+i+1\n",
    "\n",
    "i = t + 1 where t = len(SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all words in the corpus and how many times they appear\n",
    "def dictionary_count(corpus):\n",
    "    word_counts = {}\n",
    "    for i in corpus:\n",
    "        ls = corpus.get(i)\n",
    "        saver = Counter(ls)\n",
    "        word_counts[i] = saver\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_word_freq(word, corpus_words_count=corpus_words_count):\n",
    "    count = 0\n",
    "    for filename in corpus_words_count:\n",
    "        cor = corpus_words_count.get(filename)\n",
    "        if cor.get(word) != None:\n",
    "            count += cor.get(word)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns sub corpus as a string\n",
    "def get_sub_corpus(pronoun):\n",
    "    corpus_dict = {}\n",
    "    ftr_dir='../all-en-ftrs/Words/' + pronoun\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    words = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        for j in read_ftr[\"text\"]:\n",
    "            for k in j:\n",
    "                if k not in exclude:\n",
    "                    words.append(k)\n",
    "           \n",
    "    #corpus_dict[pronoun] = words\n",
    "    return words\n",
    "\n",
    "def get_sub_corpus_length(subcorpus):\n",
    "    return len(subcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_corpus():\n",
    "    \n",
    "    # file_list = file_to_path(path, 'txt')\n",
    "    corpus_dict = {}\n",
    "    #file_path = [x[0] for x in os.walk(path)]\n",
    "\n",
    "    #for ftr_dir in file_path[1:]:\n",
    "        # Gets pronoun from file name\n",
    "        #pronoun = ftr_dir[21:]\n",
    "\n",
    "        # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "\n",
    "    ftr_dir='../all-en-ftrs/'\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "    \n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        words = []\n",
    "        for j in read_ftr[\"text\"]:\n",
    "            for k in j:\n",
    "                if k not in exclude:\n",
    "                    words.append(k)\n",
    "           \n",
    "            corpus_dict[i[15:-4]] = words\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus length = M\n",
    "def get_corpus_length(corpus):\n",
    "    total = 0\n",
    "    \n",
    "    for i in corpus:\n",
    "        total+=len(corpus[i])\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_sub_corpus(subcorpus):\n",
    "    word_counts = {}\n",
    "    saver = Counter(subcorpus)\n",
    "\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_corpus()\n",
    "corpus_length = get_corpus_length(corpus)\n",
    "corpus_words_count = dictionary_count(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"you\", \"i\", \"they\", \"we\", \"trump\", \"cant\",\"youd\", \"want\",\"theyll\",\"amp\",\"me\", \"my\", \"que\", \"youve\", \"must\", \"wed\", \"well\", \"would\", \"theyd\", \"youll\",\"para\", \"como\",\"dont\", \"pero\", \"u\", \"ive\", \"ill\", \"id\", \"trump\", \"theyre\", \"theyve\", \"weve\", \"realdonaldtrump\",\"youre\", \"hundred\", \"thousand\", \"como\", \"pero\", \"im\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_spec(pronoun, subcorpus_words, subcorpus, subcorpus_length, subcorpus_no_duplicates):\n",
    "    subcorpus_word_freq = count_word_sub_corpus(subcorpus)\n",
    "    # count all words in subcorpus\n",
    "    word_tpl = []\n",
    "    \n",
    "    for w in tqdm(subcorpus_no_duplicates):\n",
    "            sc_w = subcorpus_word_freq.get(w)\n",
    "            c_w = 0\n",
    "\n",
    "            for i in corpus_words_count:\n",
    "                c = corpus_words_count.get(i)\n",
    "                cw = c.get(w)\n",
    "                if cw != None:\n",
    "                    c_w+=cw\n",
    "            if sc_w != None:\n",
    "                    word_tpl.append((w, lexical_specificity.lexical_specificity(corpus_length, subcorpus_length, c_w, sc_w)))\n",
    "\n",
    "    return word_tpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_words(pronoun):\n",
    "    ftr_dir='../all-en-ftrs/Words/' + pronoun\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    flat_list = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "        flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247523/247523 [00:04<00:00, 57558.26it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_I = get_sub_corpus('I')\n",
    "subcorpus_no_duplicates_I = list( dict.fromkeys(subcorpus_I) )\n",
    "subcorpus_length_I = get_sub_corpus_length(subcorpus_I)\n",
    "\n",
    "ls_I = get_lexical_spec('I', get_list_of_words('I'), subcorpus_I, subcorpus_length_I,subcorpus_no_duplicates_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105918/105918 [00:01<00:00, 68419.42it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_me = get_sub_corpus('Me')\n",
    "subcorpus_no_duplicates_me = list( dict.fromkeys(subcorpus_me) )\n",
    "subcorpus_length_me = get_sub_corpus_length(subcorpus_me)\n",
    "\n",
    "ls_me = get_lexical_spec('Me', get_list_of_words('Me'), subcorpus_me, subcorpus_length_me, subcorpus_no_duplicates_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72408/72408 [00:01<00:00, 70632.66it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_My = get_sub_corpus('My')\n",
    "subcorpus_no_duplicates_my = list( dict.fromkeys(subcorpus_My) )\n",
    "subcorpus_length_My = get_sub_corpus_length(subcorpus_My)\n",
    "\n",
    "ls_my = get_lexical_spec('My', get_list_of_words('My'), subcorpus_My, subcorpus_length_My, subcorpus_no_duplicates_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145171/145171 [00:02<00:00, 58430.32it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_they = get_sub_corpus('They')\n",
    "subcorpus_no_duplicates_they = list( dict.fromkeys(subcorpus_they) )\n",
    "subcorpus_length_they = get_sub_corpus_length(subcorpus_they)\n",
    "\n",
    "ls_they = get_lexical_spec('They', get_list_of_words('They'), subcorpus_they, subcorpus_length_they,subcorpus_no_duplicates_they)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72482/72482 [00:01<00:00, 70856.03it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_us = get_sub_corpus('Us')\n",
    "subcorpus_no_duplicates_us = list( dict.fromkeys(subcorpus_us) )\n",
    "subcorpus_length_us = get_sub_corpus_length(subcorpus_us)\n",
    "\n",
    "ls_us = get_lexical_spec('Us', get_list_of_words('Us'), subcorpus_us, subcorpus_length_us,subcorpus_no_duplicates_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115242/115242 [00:01<00:00, 64020.33it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_We = get_sub_corpus('We')\n",
    "subcorpus_no_duplicates_We = list( dict.fromkeys(subcorpus_We) )\n",
    "subcorpus_length_we= get_sub_corpus_length(subcorpus_We)\n",
    "\n",
    "ls_we = get_lexical_spec('We', get_list_of_words('We'), subcorpus_We, subcorpus_length_we,subcorpus_no_duplicates_We)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216850/216850 [00:04<00:00, 50261.47it/s]\n"
     ]
    }
   ],
   "source": [
    "subcorpus_you = get_sub_corpus('You')\n",
    "subcorpus_no_duplicates_you = list( dict.fromkeys(subcorpus_you) )\n",
    "subcorpus_length_you = get_sub_corpus_length(subcorpus_you)\n",
    "\n",
    "ls_you = get_lexical_spec('You', get_list_of_words('You'), subcorpus_you, subcorpus_length_you, subcorpus_no_duplicates_you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_100_words(pronoun):\n",
    "    # adjustable file in ftr format\n",
    "    ftr_dir = '../all-en-ftrs/Words/' + pronoun\n",
    "\n",
    "    # Uses the json_dir variable to navigate to the dataset directory and 'get' all json files\n",
    "    ftr_pattern = os.path.join(ftr_dir, '*.ftr')\n",
    "\n",
    "    file_list = glob.glob(ftr_pattern)\n",
    "\n",
    "    # reads contents of save_dir.ftr\n",
    "    flat_list = []\n",
    "    for i in file_list:\n",
    "        read_ftr = pd.read_feather(i, columns=None, use_threads=True)\n",
    "        ls = read_ftr[\"text\"].tolist()\n",
    "\n",
    "        flat_list = flat_list + [item for sublist in ls for item in sublist]\n",
    "\n",
    "        saver = Counter(\" \".join(flat_list).split()).most_common(200)\n",
    "        return saver[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to combine the 100 most common words in all files and exclude them from all results\n",
    "exclude_i = most_common_100_words('I')\n",
    "exclude_me = most_common_100_words('Me')\n",
    "exclude_my = most_common_100_words('My')\n",
    "exclude_they = most_common_100_words('They')\n",
    "exclude_us = most_common_100_words('Us')\n",
    "exclude_we = most_common_100_words('We')\n",
    "exclude_you = most_common_100_words('You')\n",
    "\n",
    "exclusion= []\n",
    "exclusion += exclude_i\n",
    "exclusion += exclude_me\n",
    "exclusion += exclude_my\n",
    "exclusion += exclude_they\n",
    "exclusion += exclude_us\n",
    "exclusion += exclude_we\n",
    "exclusion += exclude_you\n",
    "exclusion += exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which gets orders the lexical specificity tuples and returns the 20 highest scoring.\n",
    "# The words must not be in the exclusion list and must be between 4 and 12 characters.\n",
    "\n",
    "def top_20_ls(list_of_tuples):\n",
    "    sorted_ls = []\n",
    "    filtered_data = []\n",
    "    for i in list_of_tuples:\n",
    "        if i[0] not in exclusion and len(i[0]) >= 4 and len(i[0])<12:\n",
    "            filtered_data.append(i)\n",
    "\n",
    "    sorted_by_second = sorted(filtered_data, reverse=True, key=lambda tup: tup[1])\n",
    "\n",
    "    sorted_ls += sorted_by_second[:20]\n",
    "\n",
    "    return sorted_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_I = top_20_ls(ls_I)\n",
    "top_20_me = top_20_ls(ls_me)\n",
    "top_20_my = top_20_ls(ls_my)\n",
    "top_20_they = top_20_ls(ls_they)\n",
    "top_20_us = top_20_ls(ls_us)\n",
    "top_20_we = top_20_ls(ls_we)\n",
    "top_20_you = top_20_ls(ls_you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which prints the top 20 data\n",
    "def data_printer(data):\n",
    "    for i in data:\n",
    "        j,k = i\n",
    "        print(j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_20_I)\n",
    "print('\\n')\n",
    "print(top_20_me)\n",
    "print('\\n')\n",
    "print(top_20_my)\n",
    "print('\\n')\n",
    "print(top_20_they)\n",
    "print('\\n')\n",
    "print(top_20_us)\n",
    "print('\\n')\n",
    "print(top_20_we)\n",
    "print('\\n')\n",
    "print(top_20_you)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
