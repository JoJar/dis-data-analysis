{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from nltk.stem.porter import PorterStemmer\r\n",
    "import nltk\r\n",
    "import re\r\n",
    "from num2words import num2words\r\n",
    "import plotly.graph_objects as go"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def preprocess(text):\r\n",
    "    text = text.lower()\r\n",
    "    text = remove_apostrophes(text)\r\n",
    "    text = remove_punctuation(text)\r\n",
    "    text = remove_single_chars(text)\r\n",
    "    text = remove_links(text)\r\n",
    "    text = remove_aux_verbs(text)\r\n",
    "    text = remove_search_terms(text)\r\n",
    "    text = numbers_to_words(text)\r\n",
    "    text = tokenize(text)\r\n",
    "    text = remove_stop_words(text)\r\n",
    "    text = stemmer(text)\r\n",
    "    \r\n",
    "    return text"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['achowardwriter', 'tappytapin', 'mainerbee', 'naomiaklein', 'whats', 'like', 'youre', 'expert', 'politician', 'done', 'favored', 'politician', 'done', 'make', 'better', 'country', 'world']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "def remove_stop_words(text):\r\n",
    "    stop_words = ['myself', 'our', 'ours', 'ourselves', 'your', 'yours',\r\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\r\n",
    "    \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself',\r\n",
    "    'them', 'their', 'theirs', 'themselves', 'what', 'which',\r\n",
    "    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am',\r\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\r\n",
    "    'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\r\n",
    "    'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\r\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\r\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to',\r\n",
    "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\r\n",
    "    'again', 'further', 'then', 'once', 'here', 'there', 'when', \r\n",
    "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\r\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\r\n",
    "    'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\r\n",
    "    'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're',\r\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\r\n",
    "    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\r\n",
    "    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\r\n",
    "    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n",
    "\r\n",
    "    filtered_words = [word for word in text if word not in stop_words]\r\n",
    "    return filtered_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'They', 'We', 'My', 'Us', 'You', 'Me']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def numbers_to_words(text):\r\n",
    "    num_list = re.findall(r'\\d+', text)\r\n",
    "    str_num_list = []\r\n",
    "\r\n",
    "    for num in num_list:\r\n",
    "            str_with_punct = num2words(num)\r\n",
    "            str_without_punct = re.sub('[^A-Za-z0-9 ]+', '', str_with_punct)\r\n",
    "            str_num_list.append(str_without_punct)\r\n",
    "    \r\n",
    "    for i, num in enumerate(num_list):\r\n",
    "        text = text.replace(str(num), str_num_list[i])\r\n",
    "  \r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def remove_single_chars(text):\r\n",
    "    txt = text.split()\r\n",
    "    updated_text = \"\"\r\n",
    "    for word in txt:\r\n",
    "        if len(word) > 1 or word.lower() == \"i\":\r\n",
    "            updated_text = updated_text + \" \" + word\r\n",
    "    return updated_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Remove Concordance\r\n",
    "def remove_concordance(filename, pronoun):\r\n",
    "    lines = \"\"\r\n",
    "    with open(filename, 'r', encoding='utf8') as f:\r\n",
    "        for line in f.readlines():\r\n",
    "                # Takes the combined line from the concordance.\r\n",
    "                line = line[line.find(\"line=\"):].strip(\"line=\")\r\n",
    "                line = re.sub('[^A-Za-z0-9\\' ]+', '', line)\r\n",
    "                lines += line\r\n",
    "    # Preprocesses each line and adds to new file\r\n",
    "    writeToFile('../textFiles/Pronoun/clean/stripped-concordance-' + pronoun + '.txt', preprocess(lines))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Remove apostrophes\r\n",
    "def remove_apostrophes(text):\r\n",
    "    text = text.replace(\"'\", \"\")\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Remove Auxillary Verbs\r\n",
    "def remove_aux_verbs(stripped_string):\r\n",
    "    aux_verbs = [\"have\",\"be\",\"been\",\"being\"]\r\n",
    "    list_string = stripped_string.split()\r\n",
    "\r\n",
    "    cleaned_words = [word for word in list_string if word.lower() not in aux_verbs]\r\n",
    "    results = ' '.join(cleaned_words)\r\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Remove Links\r\n",
    "def remove_links(text):\r\n",
    "    text = re.sub(r'http\\S+', '', text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Tokenizes words in document text\r\n",
    "def tokenize(text):\r\n",
    "    tokens = nltk.word_tokenize(text)\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Stems tokenized words\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    " \r\n",
    "\r\n",
    "def stemmer(text):\r\n",
    "    lemmatizer = WordNetLemmatizer()\r\n",
    "    stems = []\r\n",
    "    for item in text:\r\n",
    "        stems.append(lemmatizer.lemmatize(item))\r\n",
    "        #stems.append(PorterStemmer().stem(item))\r\n",
    "    return stems"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def remove_punctuation(text):\r\n",
    "    words = []\r\n",
    "    for word in text:\r\n",
    "        w = re.sub(r'([^\\w\\s\\d]|_)','',word)\r\n",
    "        words.append(w)\r\n",
    "    words = \"\".join(words)\r\n",
    "\r\n",
    "    return words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Remove Search Terms\r\n",
    "def remove_search_terms(stripped_string):\r\n",
    "    stripped_string = stripped_string.replace(\"conspiracy\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"propaganda\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"Trump\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fake\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"news\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"realDonald\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"misinformation\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fake news\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"disinformation\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"active measures\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"subversion\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"interference\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"influence\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"deep state\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fabrication\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"manipulate\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"deceive\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"useful idiots\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"mainstream media\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"populism\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"untrustworthy\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"made-up\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"bogus\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"inaccurate\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"doctored\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"fact Checking\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"eu troll\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"europe troll\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european false\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european fraud\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european hoax\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european lies\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european rumours\", \"\").strip()\r\n",
    "    stripped_string = stripped_string.replace(\"european troll\", \"\").strip()\r\n",
    "    return stripped_string"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "6c91f7360f31a60595970ce0519c225953292631b532536816811087a825ec9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}