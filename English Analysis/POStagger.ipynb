{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c06748-e2c9-4aab-a1c7-dfff3b5c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1845b71-c815-414e-a0e4-4dad27ce929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(filename, left_tokenize, right_tokenize, query):\n",
    "    obj=[query],left_tokenize,right_tokenize\n",
    "\n",
    "    with open(filename, 'a', encoding='utf8') as fin:\n",
    "        fin.write(str(obj) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4ae59e-29c3-4eb5-a5e6-fa6cf44c2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new file, takes the base concordance file and runs POS tagging.\n",
    "# This code strips the concordance down to left and right of the query (word concordance query)\n",
    "def stripConcordance(query):\n",
    "    stripped_string = \"\".lower()\n",
    "    with open('concordance_'+ query + '.txt', 'r', encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            # Strips the left and right side of the concordance of unwanted parts.\n",
    "            left = line[line.find(\"left_print=\"):line.find(\"right_\")].strip(\"left_print=\")\n",
    "            left_re = re.sub('[^A-Za-z0-9 ]+', '', left)\n",
    "            right = line[line.find(\"right_print=\"):line.find(\"line=\")].strip(\"right_print=\")\n",
    "            right_re = re.sub('[^A-Za-z0-9 ]+', '', right)\n",
    "            \n",
    "            # Combines the left and right side of the concordance into a single string.\n",
    "            stripped_string += left_re\n",
    "            stripped_string += \" \"\n",
    "            stripped_string += right_re\n",
    "            \n",
    "            left_tokenize = nltk.pos_tag(nltk.word_tokenize(left_re))\n",
    "            right_tokenize = nltk.pos_tag(nltk.word_tokenize(right_re))\n",
    "            writeToFile('POS-concordance-'+ query +'.txt', left_tokenize, right_tokenize, query)\n",
    "        \n",
    "        return posTag(stripped_string, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25612dfe-bfc3-4cbf-b2d6-c99bcd3b7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes words used to obtain data as well as any words that have snuck into the concordance.\n",
    "def posStripper(stripped_string):\n",
    "    stripped_string = stripped_string.replace(\"conspiracy\", \"\")\n",
    "    stripped_string = stripped_string.replace(\"propaganda\", \"\")\n",
    "    stripped_string = stripped_string.replace(\"misinformation\", \"\")\n",
    "    stripped_string = stripped_string.replace(\"https\", \"\")\n",
    "    return stripped_string       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e9222f5-45ec-4633-8bf3-aa2059348967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(stripped_string, query):\n",
    "    # calls function to remove certain words used to obtain data.\n",
    "    stripped_string = posStripper(stripped_string)\n",
    "    \n",
    "    # POS tag the entire concordance.\n",
    "    token = nltk.pos_tag(nltk.word_tokenize(stripped_string))\n",
    "    pos_frq = nltk.FreqDist(tag for (word, tag) in token)\n",
    "    nouns = getNouns(token, query)\n",
    "    verbs = getVerbs(token, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e613951-0683-425f-8056-6577c91a5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all nouns and return the 20 most common\n",
    "def getNouns(token, query):\n",
    "    most_common_nouns = []\n",
    "    for i, j in token:\n",
    "        if 'NN' in j and len(i) > 4:\n",
    "            most_common_nouns.append(i)\n",
    "    noun_frq = nltk.FreqDist(noun for noun in most_common_nouns)\n",
    "    most_common_nouns = noun_frq.most_common(20)\n",
    "    print(query + \" NOUNS\")\n",
    "    return most_common_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4a12e9-94e5-4485-a87e-43652aeedecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all verbs and return the 20 most common\n",
    "def getVerbs(token, query):\n",
    "    most_common_verbs = []\n",
    "    for i, j in token:\n",
    "        if 'VB' in j and len(i) > 4:\n",
    "            most_common_verbs.append(i)\n",
    "    verb_frq = nltk.FreqDist(verb for verb in most_common_verbs)\n",
    "    most_common_verbs = verb_frq.most_common(20)\n",
    "    print(query + \" VERBS\")\n",
    "    return most_common_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7739ab64-1bd3-4a63-83de-227d10acb253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I NOUNS\n",
      "[('people', 1509), ('Trump', 811), ('realDonaldTrump', 728), ('media', 690), ('theories', 628), ('theory', 501), ('state', 378), ('virus', 378), ('thing', 371), ('guess', 368), ('China', 335), ('theorist', 313), ('think', 307), ('anything', 296), ('something', 273), ('tweet', 258), ('things', 252), ('someone', 234), ('anyone', 233), ('President', 226)]\n",
      "I VERBS\n",
      "[('think', 2290), ('believe', 1341), ('going', 595), ('thought', 534), ('watch', 450), ('being', 410), ('saying', 378), ('agree', 319), ('spreading', 314), ('doing', 308), ('watching', 294), ('understand', 259), ('heard', 236), ('trying', 236), ('getting', 209), ('talking', 196), ('watched', 196), ('remember', 196), ('wonder', 193), ('listen', 192)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"I\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0423693c-c41d-4c94-9d3f-379a765d4036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me NOUNS\n",
      "[('people', 260), ('theories', 172), ('Trump', 167), ('theory', 122), ('theorist', 107), ('media', 106), ('Please', 92), ('realDonaldTrump', 83), ('virus', 83), ('someone', 81), ('thing', 73), ('state', 67), ('please', 61), ('President', 56), ('China', 56), ('something', 55), ('country', 54), ('video', 51), ('anyone', 49), ('anything', 49)]\n",
      "me VERBS\n",
      "[('makes', 168), ('believe', 119), ('think', 111), ('telling', 75), ('going', 69), ('being', 65), ('trying', 63), ('spreading', 57), ('saying', 55), ('making', 51), ('blocked', 46), ('talking', 43), ('kidding', 43), ('sending', 41), ('tells', 41), ('doing', 41), ('called', 36), ('spread', 35), ('seems', 35), ('started', 33)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"me\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "57d0d168-7f2e-4bb9-a921-043c9e3a652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you NOUNS\n",
      "[('people', 2339), ('realDonaldTrump', 1667), ('Trump', 1330), ('media', 1076), ('Thank', 953), ('China', 573), ('state', 546), ('virus', 529), ('President', 520), ('truth', 511), ('disinformation', 478), ('anything', 476), ('facts', 460), ('country', 421), ('world', 410), ('nothing', 401), ('something', 398), ('thing', 394), ('theories', 392), ('everything', 374)]\n",
      "you VERBS\n",
      "[('think', 1761), ('believe', 1287), ('spreading', 1176), ('doing', 738), ('going', 635), ('being', 589), ('spread', 531), ('saying', 415), ('talking', 405), ('lying', 354), ('trying', 340), ('makes', 318), ('understand', 308), ('watch', 298), ('called', 290), ('listen', 267), ('realDonaldTrump', 266), ('telling', 244), ('getting', 228), ('realize', 218)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e9f43937-f4dd-4c4d-bd07-b029a1310b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we NOUNS\n",
      "[('people', 731), ('Trump', 455), ('media', 389), ('realDonaldTrump', 306), ('China', 273), ('country', 232), ('state', 230), ('virus', 225), ('NEVER', 217), ('world', 206), ('EVERYONE', 197), ('DemocratsNOW', 194), ('TWEET', 194), ('REPUBLICAN', 194), ('truth', 188), ('President', 172), ('thing', 166), ('information', 147), ('People', 145), ('facts', 137)]\n",
      "we VERBS\n",
      "[('believe', 305), ('think', 292), ('going', 274), ('being', 217), ('doing', 187), ('testing', 169), ('spreading', 147), ('trust', 115), ('listen', 113), ('saying', 110), ('watch', 95), ('fight', 95), ('getting', 93), ('spread', 86), ('forget', 84), ('realDonaldTrump', 84), ('talking', 83), ('continue', 76), ('expect', 75), ('start', 73)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"we\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "720272ec-6621-4b3d-9070-72f561ee1360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they NOUNS\n",
      "[('people', 1211), ('media', 655), ('Trump', 641), ('disinformation', 539), ('China', 430), ('virus', 367), ('realDonaldTrump', 331), ('state', 295), ('world', 283), ('truth', 260), ('anything', 255), ('Twitter', 245), ('nothing', 237), ('everything', 206), ('State', 205), ('money', 205), ('country', 202), ('President', 192), ('ANGEL', 179), ('government', 155)]\n",
      "they VERBS\n",
      "[('think', 594), ('believe', 517), ('doing', 388), ('expose', 369), ('going', 318), ('being', 271), ('spread', 251), ('spreading', 243), ('saying', 220), ('trying', 198), ('lying', 159), ('getting', 155), ('called', 136), ('using', 126), ('telling', 119), ('report', 110), ('started', 109), ('watch', 106), ('making', 99), ('talking', 99)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"they\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7e156-f87f-4756-9a42-6d6416bfe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=go.Bar(y=[2, 3, 1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e196d265-35b7-4996-b6b6-863fe5fb69ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly;\n",
    "plotly.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "708926d4-4c79-4638-95c6-89d609554b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us NOUNS\n",
      "[('China', 431), ('Trump', 303), ('media', 254), ('people', 243), ('virus', 201), ('coronavirus', 151), ('government', 127), ('state', 122), ('world', 120), ('truth', 112), ('State', 100), ('President', 87), ('president', 84), ('Please', 82), ('COVID19', 81), ('realDonaldTrump', 80), ('disinformation', 77), ('economy', 74), ('country', 72), ('theories', 69)]\n",
      "us VERBS\n",
      "[('telling', 132), ('going', 126), ('think', 119), ('trying', 112), ('believe', 103), ('trust', 102), ('choose', 90), ('giving', 81), ('doing', 79), ('wants', 70), ('being', 70), ('spreading', 65), ('tells', 58), ('escalates', 55), ('blame', 53), ('killing', 50), ('spread', 49), ('using', 47), ('working', 43), ('saying', 40)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(stripConcordance(\"us\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f077b93-3e4f-4dba-a21a-0ecf20cfe2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a57201-755c-4b69-849b-368ece730d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference for nltk multi processing\n",
    "#https://datascience.blog.wzb.eu/2017/06/19/speeding-up-nltk-with-parallel-processing/\n",
    "corpus = {f_id: nltk.corpus.gutenberg.raw(f_id)\n",
    "          for f_id in nltk.corpus.gutenberg.fileids()}\n",
    "\n",
    "def tokenize_and_pos_tag(pair):\n",
    "    f_id, doc = pair\n",
    "    return f_id, nltk.pos_tag(nltk.word_tokenize(doc))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # automatically uses mp.cpu_count() as number of workers\n",
    "    # mp.cpu_count() is 4 -> use 4 jobs\n",
    "    with mp.Pool() as pool:\n",
    "        tokens = pool.map(tokenize_and_pos_tag, corpus.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2510510-7b93-4bed-99fc-52332fd73dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
